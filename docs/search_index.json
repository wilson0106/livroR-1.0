[["index.html", "Introdução à análise de dados em R: Uma abordagem ambiental Prefácio Estrutura do livro Agradecimentos", " Introdução à análise de dados em R: Uma abordagem ambiental Wilson Souza &amp; Thiago Couto Prefácio Estrutura do livro Este livro é composto de 4 capítulos, com diversos exemplos e todos com exercícios. O capítulo 1 traz uma breve explicação sobre a linguagem R, as vantagens do uso de sua interface gráfica RStudio, assim como do uso dessa ferramenta para análise de dados. Iremos, ainda neste capítulo, explorar as principais guias do RStudio, como elas estão organizadas e quais as suas principais funcionalidades. O capítulo 2 vem demonstrar como se dá o processo de importação de um conjunto de dados para o RStudio por diferentes meios (linhas de comando e interface gráfica), Fazer a verificação e ajuste dos dados, sumarizar os dados estatisticamente tanto por suas métricas quanto por gráficos, usando os pacotes básicos do R. O capítulo 3 traz a análise do qui-quadrado, tanto para ajuste das frequências quanto para o teste de independência e é apresentado brevemente a construção de suas hipóteses nulas e em detalhes o seu resultado. O capítulo 4 traz a análise do teste-t para uma amostra, duas amostras e pareado os quais, também, tem suas hipóteses apresentadas e detalhamento de seus resultados e a análise de seus pressupostos. Ao fim do livro é disponibilizado um apêndice com a resposta para os exercícios. Alumas marcações são feitas no livro para melhor apresentação, tais como: Nomes de funções estão em azul; Nomes dos pacotes estão em marrom; Observações estão em vermelho; Objetos estão em negrito; Argumentos estão entre aspas; Operadores matemáticos estão em negrito e entre aspas; Nomes em inglês estão em itálico; Classes dos objetos estão em itálico1; Agradecimentos Em primeiro lugar só podemos agradecer a nossa família, pois são quem estão ao nosso lado a todo instante nos dando suporte. Em segundo lugar gostaríamos de agradecer a todos os professores pois foram eles que nos deram a base necessária para que esse livro viesse a existir. Em terceiro lugar gostaríamos de agradecer ao Rodrigo Luiz Lisboa Leitão por seus valiosos comentários e revisão dos capítulos 1 e 2. Por último e não menos importante gostaríamos de agradecer a vocês que estão utilizando o conhecimento presente aqui, porque é para vocês que este livro foi construído. pois são nomes em inglês e não iremos detalha-las só reforça-las quando fatores↩ "],["sobre-os-autores.html", "Sobre os autores", " Sobre os autores Wilson Souza Possui graduação em Licenciatura Plena em Ciências Biológicas pela Universidade Federal do Rio de Janeiro (UFRJ) com dupla diplomação com a Universidade de Coimbra (UC) - Portugal, Bacharelado em Biologia Marinha pela Universidade Federal do Rio de Janeiro (UFRJ) e Mestrado em Oceanografia Biológica pela Universidade Federal do Rio Grande (FURG). Desde 2012 trabalha em projetos de monitoramento de populações de camarões com foco em aspectos ecológicos e pesqueiros e em projetos de ecologia do benthos marinho. Aprendeu a linguagem R durante o mestrado e desde então tem se dedicado a ampliar seus conhecimentos em análise de dados voltado para monitoramento ambiental e aspectos ecológicos do benthos marinho. Thiago Couto Possui graduação em Ciências Biológicas pela Universidade Santa Úrsula, Mestrado em Ecologia pela Universidade de Coimbra – Portugal, Doutorado em Biociências com especialização em Ecologia Marinha também pela Universidade de Coimbra – Portugal e Pós-Doutorado em Oceanografia Biológica pela Universidade Federal do Rio de Janeiro. Em sua vida acadêmica já trabalhou com avaliação da qualidade ambiental usando organismos bentônicos, sequestro de carbono em vegetação de marisma, monitoramento ambiental, entre outras coisas. Nos últimos anos têm se dedicado ao monitoramento ambiental de costões rochosos e análise de risco de espécies invasoras. "],["r.html", "Capítulo 1 R 1.1 O que é? 1.2 Porque usar o R para análise de dados? 1.3 O RStudio 1.4 Exercícios", " Capítulo 1 R 1.1 O que é? É uma linguagem de programação derivada da linguagem S (1976) e desenvolvida na Bell Laboratories (Atualmente Lucen Technologies) pelo estatístico John McKinley Chambers e colaboradores a qual no início da década de 90 foi implementada pelos estatísticos Ross Ihaka e Robert Gentleman e liberada oficialmente como linguagem R em 1995, sendo a primeira versão estável liberada em 2000 e tem sido constantemente atualizada. Atualmente tem se tornado a principal ferramenta para análise de dados, tanto em meio acadêmico quanto empresarial, principalmente devido ao seu caráter open-source (código aberto). Para maiores informações acesse o site do projeto R em: https://www.r-project.org/about.html. 1.2 Porque usar o R para análise de dados? A estatística é uma ferramenta essencial para entendermos melhor os ecossistemas que estudamos. Com ela podemos compreender melhor os dados que coletamos, sejam dados biológicos ou ambientais tanto no âmbito de organismos microscópicos como macroscópicos. Existe uma grande variedade de programas estatísticos que podem satisfazer nossas necessidades de análise dos nossos dados (Statistica, SPSS, past, Primer-E etc). Então, porque devemos usar o programa R? Primeiramente, o R é gratuito, enquanto a maioria dos outros programas disponíveis no mercado não são baratos podendo a chegar a valores próximo de $6000,00 doláres a assinatura por ano, como no caso do programa SPSS. Em segundo lugar, no R é possível fazer todas as análises que os outros programas fazem, portanto, não é necessário ter mais de um programa para se realizar diversas análises. Vocês podem estar se perguntando se terão que aprender programação para trabalhar com R. A resposta é sim e não. As análises feitas no R são realizadas através de linhas de comando como em uma linguagem de programação, porém modificada, de mais fácil compreensão e intuítiva. Todavia, nosso objetivo com esta obra é facilitar o seu aprendizado desta linguagem para o universo da análise de dados, por meio de questões e exemplos pertinentes a área ambiental. Por outro lado, é preciso saber como o R se estrutura e funciona, pois um pequeno erro na escrita pode fazer com que a análise não funcione adequadamente. Mas não se preocupe iremos tentar facilitar para você. Portanto, neste livro mostraremos desde como se importar uma tabela até fazer algumas análises estatísticas e gráficos que são vastamente utilizados na área ambiental. 1.3 O RStudio O RStudio foi construído especificamente para a linguagem R e é um ambiente de desenvolvimento integrado (no inglês IDE) (Figura 1.1). Ele fornece uma interface gráfica (visa facilitar ao usuário o uso das operações executadas) mais amigável para os usuários em geral, principalmente para quem está iniciando no R. Além disso ele fornece características e recursos importantes para o universo da programação, tais como: syntax highlighthing que consiste em uma marcação diferenciada que nos sinaliza por meio de cores e outros atributos estéticos (ex.: texto em itálico) o que está sendo escrito, indicando se é um objeto, função ou argumento, por exemplo; code completion quando digitamos três caracteres de um objeto, função ele sinaliza os demais elementos presentes em sua memória que contém esses três caracteres; smart indentation o qual faz com que o texto que estamos digitando pule de linha automaticamente ao chegarmos no final do espaço porém ele mantém relação com a linha anterior2; Execute R code directly from the source editor neste caso ele tem uma guia destinada ao seu script3 o qual permite executa-lo diretamente dele e o resultado irá aparecer em outra guia denominada console; Quickly jump to function definitions executada, em Windows e Linux, pelo atalho \\(ALT + SHIFT + G\\) e que permite acessar mais rapidamente uma determinada linha do script; Integrated R help and documentation o qual traz consigo, em sua IDE, uma guia destinada a ajuda. Além destas ele traz diversas outras funcionalidades que tornam o seu uso diário mais acessível e facilitado. Mais detalhes sobre acesse https://rstudio.com. Figura 1.1: Ambiente RStudio. O primeiro passo para que possamos compreender o R e o RStudio e o que podemos fazer com ele consiste em entender a sua estrutura como ela está organizada e o que ela significa. Uma vez compreendida sua estrutura organizacional e funcionalidade poderemos mergulhar no mundo das análises de dados dentro deste novo ambiente. Para isso veremos as principais guias do RStudio, como elas estão organizadas e quais as suas principais funcionalidades. Script: Guia utilizada para escrevermos o nosso código (comandos que executarão uma determinada tarefa) e comentários. Para executar os comandos escritos neste ambiente devemos deixar o cursor na linha que queremos executar e utilizar os atalhos do teclado \\((CTRL + ENTER)\\) ou \\((CTRL + R)\\) ou clicar em “Run” (Figura 1.2). Todo comando executado irá retornar uma saída na guia do “console” (ver item 2). Se o comando executado indicar que o resultado será guardado em um objeto4, este irá aparecer na guia “environment” (ver item 3). Para salvar o script escrito você deve utilizar a opção File na “barra de menus” e clicar em Save ou usar o atalho \\((CTRL + S)\\) ou clicar no icone de “disquete” no canto superior esquerdo desta guia; Figura 1.2: Guia do RStudio referente ao ambiente onde o script será desenvolvido. Console: Guia que indica o resultado do código executado na guia referente ao script. Nesta guia também pode ser escrito o código, porém ao pressionar ENTER o código é executado e o resultado é indicado na linha imediatamente inferior (Figura 1.3); Figura 1.3: Guia do RStudio referente ao ambiente console onde o resultado dos comandos executados irão aparecer. Environment: Todo objeto5 ou função criada será indicado nesta guia. (Figura 1.4); Figura 1.4: Guia do RStudio referente ao ambiente environment, local onde estarão indicados os objetos criados. History: Guia que indica o histórico dos comandos executados (Figura 1.5); Figura 1.5: Guia do RStudio referente ao ambiente history onde estará listado todos os comandos executados. Files: Guia que permite a visualização e acesso dos arquivos do computador (Figura 1.6). Sempre defina o ambiente de trabalho6 antes de iniciar o seu projeto. As planilhas que serão utilizadas devem estar neste ambiente de trabalho, previamente definido. Embora não seja obrigatório, isso evitará problemas futuros e facilitará o seu trajeto inicial no R. As funções e códigos serão abordados, aqui, seguindo esta ideia.; Figura 1.6: Guia do RStudio referente ao ambiente files onde estará indicados os arquivos do nosso computador. Packages: Guia destinada a visualização, instalação e carregamento dos pacotes/bibliotecas do R. Os pacotes consistem em um grupo de funções, que executam uma determinada ação. Inicialmente quando se instala o R alguns pacotes são automaticamente instalados. Contudo para executar algumas funções é necessário instalar um ou mais pacotes que contenham as funções desejadas. A instalação de pacotes pode ser realizada por linha de comando como será visto no próximo capítulo ou clicando na opção Install presente no canto superior esquerdo desta guia. De tempos em tempos os pacotes sofrem alterações, realizado pelos seus desenvolvedores, por isso um hábito importante a ser tomado de tempos em tempos é clicar na opção Update ao lado da opção Install pois ela irá atualizar os pacotes que estão instalados (Figura 1.7); Figura 1.7: Guia do RStudio referente ao ambiente packages onde estarão listados todos os pacotes disponíveis no nosso computador e por onde poderemos instalar outros. Plot: Guia destinada a visualização dos gráficos executados. Algumas opções importantes desta guia é a opção Export na região central na barra de ferramentas onde é possível salvar o seu plot como imagem em formato *.jpg, *.png e *.tiff, por exemplo, assim como em *.pdf ou então a opção de copiar a imagem (Copy to Clipboard…) para colar em outro ambiente de trabalho. Uma segunda ferramenta importante é a opção do lado direito de Export que é Remove the current plot indicada por um quadrado branco com um “x” branco de fundo vermelho no canto superior esquerdo o qual remove a figura que está sendo visualizada neste ambiente, ao lado desta opção tem a figura de uma vassoura denominada Clear all Plots que deleta todas as figuras presentes neste ambiente. No lado esquerdo de Export temos a opção de Zoom o qual amplia a imagem mostrada e ao lado esquerdo desta opção temos setas indicando para direita e para esquerda, que permitem navegar por entre os plots criados durante seu trabalho (Figura 1.8); Figura 1.8: Guia do RStudio referente ao ambiente plot onde estarão os gráficos criados. Help: Guia destinada a fornecer informações sobre as funções e pacotes do R. Nesta guia há duas opções importantes na sua barra superior uma encontra-se na barra mais superior indicada por uma lupa onde é possível digitar o nome do pacote ou função em que deseja saber mais sobre, na barra abaixo desta no canto esquerdo há um outro espaço escrito Find in Topic onde ele irá buscar o termo digitado dentro do ambiente que está sendo mostrado (Figura 1.9); Figura 1.9: Guia do RStudio referente ao ambiente help onde terá informações sobre as funções e pacotes do R. 1.4 Exercícios Após escrever um comando na guia script, como fazemos para executa-lo? Pressionamos a tecla ENTER Pressionamos as teclas CTRL + ENTER Pressionamos as teclas CTRL + SPACE Pressionamos as teclas SHIFT + ENTER Presionamos as teclas CTRL + SHIFT + ENTER Após escrevermos um comando na guia console, como fazemos para executa-lo? Pressionamos a tecla ENTER Pressionamos as teclas CTRL + S Pressionamos as teclas CTRL + SPACE Pressionamos as teclas SHIFT + ENTER Presionamos as teclas CTRL + SHIFT + ENTER Ao realizarmos um gráfico, em que guia ele irá aparecer console Viewer Plots Environment Script Se desejarmos verificar os comandos previamente executados, mesmo após ter reiniciado o R, qual é a melhor forma de consulta? Verificar o console Acessar a guia history Acessar a guia environment Verificar o script Acessar a guia help Se executarmos um comando na guia script, em que guia o resultado irá aparecer? Obrigatoriamente nas guias console e environment Obrigatoriamente nas guias plots e console Obrigatoriamente na guia environment Obrigatoriamente nas guias plots e environment Obrigatoriamente na guia console Por padrão esta opção não está ativada, precisa ser ativado nas opções do programa↩ Local onde você irá escrever o código para execução de uma determinada tarefa↩ Há diferentes tipos de objetos no R, falaremos sobre eles no próximo capítulo.↩ Elemento que armazena um valor ou estrutura de dados. Todo objeto apresenta uma classe. Os objetos mais comuns são: vetor (objeto que contêm uma sequência unidimensional de elementos), matriz (Objeto bidimensional onde todos os elementos são da mesma classe), data-frame (Objeto bidimensional onde os elementos pertencentes podem ser de classes diferentes) e lista (é um vetor que agrupa, de maneira unidimensional, diferentes objetos). As classes mais comuns são: inteiro (valores quantitativos sem casa decimal), númerico (valores quantitativos com casa decimal), lógico (valores que indicam 0 ou 1, que representam respectivamente verdadeiro ou falso), fator (valores que indicam categorias) e caracter (valores que indicam texto). Vamos utiliza-los sempre que necessário, no decorrer do livro explicando-o seu significado, porém não iremos abordá-los em detalhes.↩ No próximo capítulo ensinaremos a como definir o ambiente de trabalho↩ "],["importar-organizar-e-sumarizar-os-dados.html", "Capítulo 2 Importar, organizar e sumarizar os dados 2.1 1° passo: Definir o ambiente de trabalho e importar a planilha de dados para o R 2.2 2° passo: Verificar e ajustar os dados 2.3 3° passo: Sumarizar os dados graficamente 2.4 4° passo: Sumarizar os dados numericamente 2.5 Considerações 2.6 Exercícios", " Capítulo 2 Importar, organizar e sumarizar os dados 2.1 1° passo: Definir o ambiente de trabalho e importar a planilha de dados para o R Ao se trabalhar com R a primeira pergunta que nos vem à cabeça é: Como inserir meus dados neste ambiente de trabalho? A resposta é simples e também complexa, pois depende do formato do arquivo de dados que está usando ( *.csv, *.xls, *.xlsx, *.txt entre outros). Para um mesmo formato existem inúmeras comandos possíveis para importar os dados. Não iremos especificar todas os comandos, mas os mais comuns (*.csv e *.xlsx), utilizando os comandos básicos do R e também por meio da interface gráfica do RStudio. Para importar a planilha é necessário definir o ambiente de trabalho no seu computador. Este ambiente é a pasta de seu computador onde sua planilha e seus demais dados a serem trabalhados se encontram. Podemos realizar isso via linhas de comando ou pela própria interface do RStudio. Se realizado por linhas de comandos há diferença entre o sistema operacional (Windows, Linux ou macOS) que estiver utilizando. Aqui estaremos trabalhando com o sistema operacional Windows. 2.1.1 Definindo ambiente de trabalho via linha de comando Para o sistema operacional Windows utilizamos o seguinte comando: setwd(choose.dir(path)), onde path é o caminho do diretório escolhido. Se no Windows utilizarmos apenas a linha de comando setwd(choose.dir()) sem especificar o caminho a seguinte janela irá aparecer (Figura 2.1), a partir da qual iremos selecionar a pasta onde estão os arquivos que iremos trabalhar e salvar nosso script. Figura 2.1: Janela que será aberta ao executar o comando: setwd(choose.dir()), no sistema operacional Windows. A função setwd() é responsável por selecionar o diretório a ser trabalhado. Após isso podemos importar a planilha com os dados que utilizaremos. Como resultado desta etapa podemos verificar a guia Files7, nela agora situa-se as pastas e arquivos da pasta que selecionamos e onde estará salvo nosso script quando o salvarmos. 2.1.2 Definindo ambiente de trabalho via interface gráfica Diversas formas podem ser utilizadas para definir o ambiente de trabalho, apresentaremos 3. A primeira consiste em usar o atalho do teclado \\((CTRL + SHIFT + H)\\) e nela selecionaremos o ambiente de trabalho; a segunda via consiste em acessar a guia Session ir até a opção Set Working Directory e selecionar a opção Choose Directory da mesma forma que a opção anterior a mesma janela irá aparecer; por último podemos usar a guia Files onde podemos clicar nos 3 pontos “…” que aparecem no canto superior direito da guia e buscar o ambiente/pasta que iremos utilizar e a seguir na engrenagem (More) e selecionar a opção Set as Working Directory (Figura 2.2). Figura 2.2: Guia Files indicando a opção para definir o local do computadors que será utilizado como ambiente de trabalho 2.1.3 Importando planilha via linha de comando: Primeiro construa as planilhas a seguir nos formatos *.csv (usando vírgula como separador de colunas e ponto como separador decimal) (Figura 2.3) e *.xlsx (Figura 2.4) em um editor de planilhas, como excel, libreoffice calc ou outros e a salve com o nome de planilha.csv e planilha.xlsx no diretório que definiu previamente. A seguir demonstraremos as diferenças na importação de ambas as planilhas. Figura 2.3: Planilha com os dados no formato *.csv que deve ser construída em algum editor de planilhas e salva na pasta de trabalho definida previamente Figura 2.4: Planilha com os dados no formato *.xlsx que deve ser construída em algum editor de planilhas e salva na pasta de trabalho definida previamente Digite em seu script a seguinte linha de comando: dados &lt;- read.csv(file = &quot;planilha.csv&quot;, header = TRUE, sep = &quot;,&quot;) OBS: Não se esqueça de definir previamente o ambiente de trabalho e verificar que o arquivo planilha.csv encontra-se presente nele. O comando read.csv() importou a planilha que está no formato *.csv utilizando os argumentos “file” o qual indica o nome do arquivo a ser importado; “header” o qual indica se as colunas da planilha tem nome e “sep” o qual indica qual o caracter separador das colunas. OBS: repare que há o operador “&lt;-” , esse é o operador de atribuição, ele indica que o elemento a sua direita (seja um valor ou uma função) será atribuído ao elemento que está a sua esquerda (geralmente um novo objeto). Neste caso o elemento retornado pela função read.csv(file = “planilha.csv”, header = TRUE, sep = “,”) será atribuído a um objeto chamado dados. O comando acima importou a planilha e o guardou em um objeto chamado dados que será o que nós utilizaremos de agora em diante sempre que quisermos acessar essa planilha no RStudio. Para importar arquivos que estão no formato *.xls ou *.xlsx é necessário a instalação de algum pacote8 que permita a realização desse processo. Um pacote que instalaremos aqui é o readxl. install.packages(&quot;readxl&quot;) library(readxl) dados &lt;- read_excel(path = &quot;planilha.xlsx&quot;, col_names = TRUE) Os comandos read.csv() e read_excel() são similares, embora seus argumentos sejam diferentes. O argumento “path” é similar ao “file”, o argumento “col_names” é similar ao “header”. Repare que não temos o argumento “sep”, pois em um arquivo excel as variáveis são salvas em colunas. Observe que independente da forma de importação (que é dependente do formato do arquivo *.csv ou *.xlsx) o resultado final é o mesmo (Figura 2.5). Figura 2.5: Resultado da importação do arquivo observado na guia environment. Há um objeto de nome dados, com 40 observações e 4 variáveis. 2.1.4 Importando planilha via interface gráfica: Para importar a planilha com os dados via interface do RStudio devemos acessar a guia Environment e nesta acessar a opção Import Dataset a seguir temos várias opções a qual devemos escolher referente a extensão do arquivo que vamos importar, no nosso caso será a opção From Excel (Figura 2.6). Figura 2.6: Tópicos para importação ddos dados pela via grafica. Neste momento uma janela irá abrir (Figura 2.7) e no canto superior direito haverá o nome Browse onde deveremos clicar e escolher o arquivo com o qual iremos trabalhar. Em Import options no canto inferior esquerdo haverá algumas opções, na opção name deveremos renomear para dados, para coincidir com as vias que fizemos anteriormente. As demais opções podemos deixar em branco. Repare que no canto inferior direito aparecerá um comando relativo ao processo de importação similar ao que fizemos no tópico anterior e, aparecerá também, um comando chamado View() que veremos mais adiante, para desabilita-lo basta desmarcar a caixa ao lado de mesmo nome. Conclua o processo clicando em import no canto inferior direito. Figura 2.7: Importação dos dados pela via gráfica 2.2 2° passo: Verificar e ajustar os dados Uma falha inicial comum é passar para as análises antes de verificar se os dados estão corretamente organizados, após a importação. Uma forma de previnir isso é conhecendo os seus dados e sabendo o que eles significam. Os dados, geralmente, podem ser: categóricos (ou qualitativos) ou mensuráveis (ou quantitativos). Quando categóricos temos de informar que eles consistem em fatores. Para isso vamos executar alguns comandos onde poderemos avaliar se no processo de importação as variáveis presentes na planilha de dados são reconhecidas como esperamos que seja. Ao executarmos a função str() do arquivo dados podemos ver no console o seguinte resultado str(dados) ## tibble [40 × 4] (S3: tbl_df/tbl/data.frame) ## $ ano : num [1:40] 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 ... ## $ local : chr [1:40] &quot;Ambiente 1&quot; &quot;Ambiente 1&quot; &quot;Ambiente 1&quot; &quot;Ambiente 1&quot; ... ## $ comprimento: num [1:40] 10 11 15 12 17 14 19 16 15 12 ... ## $ peso : num [1:40] 20 21 24 24 23 28 26 25 21 22 ... Seu resultado nos informa sobre o tipo de objeto que o arquivo é. Todo objeto pertence a alguma classe neste caso nosso objeto e da classe tibble ou data frame9. Também nos é mostrado as variáveis presentes na planilha dados (ano, local, comprimento e peso), cada variável é, também, um objeto e portanto tem suas classes definidas automaticamente, o que pode ser visualizado ao lado de cada variável. num = numeric (númerico), chr = character (caracter) e há outras que não estão indicadas aqui, como logic (lógica), integer (inteiro), factor (fator) entre outros. Repare que a variável ano foi indicada como numérico. Variáveis consideradas numéricas são aquelas as quais foram mensuradas. Ano, embora númerica, não é uma variável possível de ser medida, porém devido ao seu registro ter sido em valores numéricos foi assim que ela foi entendida. Portanto temos que transforma-la. Pensando no que definimos acima com variáveis categóricas e mensuráveis, ano e local podem ser classificados como variáveis categóricas. Devemos transformar em fator todas as variáveis de nossas planilha consideradas categóricas. Para isso utilizamos a função as.factor() a qual realiza esse processo que pode ser feito da seguinte forma: dados$ano &lt;- as.factor(dados$ano) dados$local &lt;- as.factor(dados$local) Repare que nesses comandos inserimos o operador “$” (cifrão). Esse é um operador importante em toda linguagem de programação e será bastante utilizado. Ele permite acessar alguma variável dentro de um objeto. Pensando no objeto dados, ao colocarmos seu nome e em seguida o operador “$” podemos acessar as variáveis dentro dele. O comando portanto é lido da seguinte forma: dados$ano &lt;-: indica que uma variável ano será criada dentro do objeto dados. as.factor(dados$ano): indica que o objeto criado consistirá na variável ano que está no objeto dados porém considerando-a como factor. Uma dúvida que pode aparecer aqui é: O que aconteceu com a variável ano que já existia dentro da planilha dados? Temos duas variáveis ano, agora? A resposta é que quando criamos um objeto ou variável com o mesmo nome de um já existente o anterior é substituído. Conduza o seguinte exemplo. Crie o objeto idade que contenha o valor de 30. idade &lt;- 30 Repare o objeto idade criado no environment. Agora crie e execute um segundo objeto de mesmo nome, porém valor diferente, e repare no enviroment. idade &lt;- 40 Reparou que ao criar um segundo objeto de mesmo nome que antes o primeiro foi apagado e substituído pelo segundo? Este é o mesmo processo que ocorreu quando realizamos a função que definiu ano e local como fatores dentro do objeto dados. Vamos observar novamente a função str() para verificar se ano e locais, agora, são fatores str(dados) ## tibble [40 × 4] (S3: tbl_df/tbl/data.frame) ## $ ano : Factor w/ 2 levels &quot;2020&quot;,&quot;2021&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ local : Factor w/ 2 levels &quot;Ambiente 1&quot;,&quot;Ambiente 2&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ comprimento: num [1:40] 10 11 15 12 17 14 19 16 15 12 ... ## $ peso : num [1:40] 20 21 24 24 23 28 26 25 21 22 ... Como pode verificar agora temos 2 variáveis como fatores e 2 como numéricas. Mais que isso, essa função nos indica ao lado dos fatores um w/2 levels que está nos dizendo que essa variável tem 2 níveis. De maneira similar, as variáveis numéricas indicam colchetes com os seguintes valores [1:40] que significa que temos 40 valores/observações/linhas. Observe a planilha dados no environment. Note que ao lado do nome dados há escrito 40obs. of 4 variables. Isto indica que temos 40 observações e 4 variáveis. Que tal agora observarmos nossa planilha? Podemos fazer isso por meio da execução função head() do nosso objeto (dados). Onde no console são mostradas as primeiras linhas de nossa planilha head(dados) ## # A tibble: 6 x 4 ## ano local comprimento peso ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2020 Ambiente 1 10 20 ## 2 2020 Ambiente 1 11 21 ## 3 2020 Ambiente 1 15 24 ## 4 2020 Ambiente 1 12 24 ## 5 2020 Ambiente 1 17 23 ## 6 2020 Ambiente 1 14 28 Outra maneira é executarmos a função View() que abre uma aba, ao lado de seu script mostrando todos os dados. View(dados) Experimente agora escrever o nome do objeto dados e executar, o que aparece? Experimente, também, clicar no nome do objeto dados, presente no environment. O que você observou? Se no primeiro caso você observou seu objeto aparecendo no console, parabéns você está indo no caminho certo. Se no segundo momento você observou que ao clicar sobre o nome dados ele executou a mesma coisa que a função View() você está indo maravilhosamente bem. Continue assim! Isto não é tudo, mas sim o necessário para começarmos a sumarizar os dados de maneira eficiente. Não se esqueça que antes de trabalharmos nossos dados temos que entender o que eles são e a que eles se referem. A análise de dados sem prévio conhecimento sobre o que eles significam pode levar a interpretações errôneas, caso as análises realizadas indiquem algum resultado, ou a muita dor de cabeça, se algum comando para uma análise não funcionar como esperado. Portanto sigam os passos precisamente, não pule etapas e busque diversificar a fonte do seu conhecimento. Pois o R é uma linguagem que como demonstramos até o momento há muitas vias possíveis de escrita para um mesmo objetivo e diferentes autores podem escrever de formas diferentes. 2.3 3° passo: Sumarizar os dados graficamente Já importamos nossa planilha e já a ajustamos. Que tal começarmos a ver a “mágica” acontecer e graficarmos nossos dados? Muitos gráficos podem ser realizados, mas lembre-se que há toda uma lógica por trás disso. Pensando nos gráficos mais comuns e úteis para nosso dia-a-dia e que nos permitem sumarizar nossos dados, temos os histogramas, gráficos de barras, de pontos, de linhas e os boxplots. Vamos então começar com esses e com o objetivo inicial de sumarizar nossos dados. 2.3.1 Histograma O histograma nos permite verificar a distribuição da frequência dos nossos dados. Por exemplo, imagine uma amostra com diversos indivíduos de um dado grupo. Caso queiramos representar as frequências relativas das diferentes faixas de peso deste grupo, podemos usar o histograma. A função hist() permite visualizar os valores da variável comprimento (Figura 2.8). Vejamos o exemplo abaixo. hist(dados$comprimento) Figura 2.8: Comprimento dos organismos da planilha dados Repare que um gráfico foi gerado na sua janela lateral, na guia plots. Perceba também que voltamos a usar o operador “$” (cifrão). Nosso gráfico não está com uma aparência muito agradável além de estar um pouco sem cor. Podemos editar e melhorar o aspecto do gráfico ao inserir cores ou outros argumentos estéticos que melhoram sua visualização e entender como ele sumariza nossos dados. hist(dados$comprimento, ylim = c(0, 10), xlim = c(9, 20), col = &quot;blue&quot;, main = &quot;Meu Histograma&quot;, xlab = &quot;Comprimento&quot;, ylab = &quot;Frequência absoluta&quot;) Figura 2.9: Comprimento dos organismos da planilha dados com barras coloridas Temos aqui o histograma (Figura 2.9) com alguns argumentos que permitiram melhorar sua visualização. Agora é possível observar a variável comprimento e sua frequência de ocorrência. Além disso podemos observar por meio desse que o valor mínimo observado é 10, que o máximo é 19 e que as frequências das diferentes classes de comprimento formadas variam entre 0 e aproximadamente 9. Será que isto que observamos está correto? Vamos ver no tópico a seguir (4° passo: Sumarizar os dados numericamente). Ainda em relação ao histograma é possível ver na função hist() que utilizamos diversos argumentos como: “xlim”, “ylim”, “col”, “main”, “xlab” e “ylab”, além de uma função chamada c(). xlim: define os limites do eixo x; ylim: define os limites do eixo y; col: define as cores de preenchimento das barras; xlab: define o nome do eixo x; ylab: define o nome do eixo y; c(): concatena (une) valores; OBS: Essa função c() é muito importante e será utilizada constantemente ao longo de seu trajeto no R. 2.3.2 Gráfico de Barras O gráfico de barras é um dos mais utilizados. Com ele podemos comparar valores de categorias diferentes. Por exemplo, dados onde temos números de indivíduos de diversas espécies podem ser representados em gráficos de barras, onde cada espécie será representada por uma barra. Se já chegamos até esse ponto vamos exercitar um pouco antes de plotar o gráfico de barras. Construa a seguinte planilha no seu editor de planilhas (Figura 2.10) e nomeie-a de abundancia. Figura 2.10: Planilha de dados de abundância. Acesse a planilha abundância. Vamos inseri-la no R com o nome abundancia. repare que retirei o acento e as letras estão em minúsculo. Sempre que trabalharmos com linguagem de programação evite acentuações, espaços, cedilhas (ç) e atente a maiúsculas e minúsculas, pois isso pode interferir no seu código OBS: Se estiver iniciando o R agora lembre-se de definir o ambiente de trabalho. abundancia &lt;- read_excel(&quot;abundancia.xlsx&quot;, col_names = TRUE) Repare que ela está no formato *.xlsx e apresenta as seguintes variáveis: mes, N e desvio. Onde N significa o número de indivíduos observados e desvio o desvio padrão. Vamos apenas conferir se está tudo certo com os dados usando a função str(). str(abundancia) ## tibble [12 × 3] (S3: tbl_df/tbl/data.frame) ## $ mes : num [1:12] 1 2 3 4 5 6 7 8 9 10 ... ## $ N : num [1:12] 100 70 50 30 35 40 60 90 110 150 ... ## $ desvio: num [1:12] 10 12 11 8 8 14 11 9 12 15 ... Vamos converter a variável mês em factor. abundancia$mes &lt;- as.factor(abundancia$mes) Repare que agora que sabemos os passos a realizar, da importação dos arquivos até este momento tudo ficou muito mais fácil e fluido e apenas realizamos três etapas, sendo uma delas apenas para conferir os dados. Seguimos então para o gráfico de barras (Figura 2.11), a fim de mostrar o número de indivíduos ao longo dos meses. Para isso utilizaremos a função barplot(). barplot(abundancia$N) Figura 2.11: Gráfico de barras com número de indivíduos da planilha abundância por mês Assim como o histograma, o simples uso da função sem argumentos gera um gráfico de difícil análise e esteticamente não muito agradável. Vamos portanto adicionar alguns argumentos já conhecidos e outros novos que permitirão a melhor análise do gráfico (Figura 2.12). barplot(abundancia$N, ylim = c(0, 200), xlab = &quot;Mês&quot;, ylab = &quot;Abundância&quot;, names.arg = abundancia$mes, col = &quot;green&quot;) Figura 2.12: Gráfico de barras com número de indivíduos da planilha abundância por mês, com barras coloridas No código acima definimos o que será graficado, como a variável N presente no objeto abundância e informamos isso por meio do operador “$” (cifrão). Ajustamos os limites do eixo “y” através do argumento “ylim” em 0 e 200, usando a função concatenar c(). Definimos os nomes dos eixos x e y por meio dos argumentos “xlab” e “ylab”, respectivamente. Definimos o nome das barras de acordo com a variável mes, também presente no objeto abundancia e por último definimos a cor das barras, usando o argumento “col”, como verde. Podemos extrair desse gráfico que a abundância dos indivíduos variam ao longo dos meses com valores menores em torno de 50 indivíduos nos meses 4, 5 e 6 e valores maiores em torno de 200 indivíduos no mês 11. 2.3.3 Gráfico de Pontos e de Linhas Quando queremos representar dados quantitativos em ambos os eixos, podemos usar tanto o gráfico de pontos quanto o de linhas. Com eles geralmente podemos verificar a relação entre as variáveis (gráfico de pontos) ou a tendência dos nossos dados em ordem cronológica (gráfico de linhas). Por exemplo, podemos representar num gráfico de linhas a variação da quantidade de indivíduos de uma espécie ao longo do ano ou os valores de precipitação ao longo de décadas. Para fazer um gráfico seja de pontos ou de linhas pelo comando básico do R (utilizamos a função plot()) precisamos indicar que ambos os eixos apresentam variáveis numéricas. Portanto teremos que modificar a variável mês de fator para número. Porém não precisamos fazer isso em um comando separado podemos aplicar a função que converte uma variável em numérica dentro da função que plota o gráfico. Vamos visualizar como isso ocorre na prática (Figura 2.13). plot(x = as.numeric(abundancia$mes), y = abundancia$N) Figura 2.13: Representação gráfica do número de indivíduos por mês em pontos Pronto, na função acima aplicamos a função plot() indicando os argumentos x e y que definem quem estará no eixo x e quem estará no eixo y, e para a variável que estará no eixo x (variável mês) adicionamos a função as.numeric(), a qual a converte em numérico apenas para a execução do plot. Porém a função plot() não realizou um gráfico de linhas como esperado, pois para isso precisamos indica-lo por meio de argumentos. Como demonstraremos a seguir (Figura 2.14). plot(x = as.numeric(abundancia$mes), y = abundancia$N, xlab = &quot;Mês&quot;, ylab = &quot;Abundância&quot;, main = &quot;Meu gráfico de linha&quot;, ylim = c(0, 200), type = &quot;l&quot;, col = &quot;green&quot;, lwd = 2) Figura 2.14: Representação gráfica do número de indivíduos por mês em um gráfico de linhas Repare que inserimos alguns argumentos novos: main: que insere o título do gráfico; type: que define que será um gráfico de linhas; lwd: que determina a espessura da linha; 2.3.4 Boxplot Podemos usar os bloxplots para verificar a tendência das amostras e a distribuição destas. Quando se tem uma pequena quantidade de amostras (por exemplo, 3 réplicas) esse gráfico não é indicado. Nele também podemos verificar se temos alguma anomalia nos dados, os chamados outliers. Vamos voltar a nossa primeira planilha dados e executar um gráfico do tipo boxplot (Figura 2.15) e ver o que ele nos informa. Para isso utilizaremos a função boxplot(). boxplot(formula = dados$comprimento ~ dados$ano) Figura 2.15: Gráfico tipo boxplot do comprimento por ano, da planilha dados Repare que a escrita para esse gráfico é ligeiramente diferente. Dentro desta função há um argumento chamado “fórmula”, onde inserimos a variável independente (variável do eixo X) e a variável dependente (variável do eixo Y). Entre ambas as variáveis há o caracter “~” (til). A forma de ler essa escrita é a seguinte: Realize um boxplot da variável comprimento do objeto dados em função da variável ano do objeto dados. O caracter til é lido como (“em função de”) Vamos adicionar alguns argumentos que permitem deixar o gráfico mais apresentável (Figura 2.16). boxplot(formula = dados$comprimento ~ dados$ano, col = c(&quot;blue&quot;, &quot;green&quot;), xlab = &quot;Ano&quot;, ylab = &quot;Comprimento&quot;, main = &quot;Meu Boxplot&quot;) Figura 2.16: Gráfico tipo boxplot do comprimento por ano com cores por ano, da planilha dados Repare que não adicionamos nenhum argumento ou função diferente do que já fizemos anteriormente. Ou seja, uma vez conhecendo a função, os argumentos por vezes se repetem entre funções com o mesmo objetivo (neste caso o objetivo é gerar um gráfico). O boxplot porém nos fornece algumas informações extras em relação ao resumo dos dados. Podemos extrair dele dados como média, mediana (segundo quartil), primeiro e terceiro quartil, assim como máximos e mínimos da variável dependente (variável do eixo y). As linhas de cada box (caixa) que se referem a cada um dos anos representam, em suas extremidades, os valores máximos e mínimos de comprimento de cada ano. A base e o topo de cada box indicam o primeiro e terceiro quartil, respectivamente, do comprimento do referido ano. A linha no interior da caixa indica a mediana (segundo quartil) e a metade do box (não marcada graficamente) indica a média. Podemos por meio desse gráfico, então, ter uma ideia do que os dados indicam quanto as medidas de tendência central e algumas medidas de dispersão em relação as categorias presentes. 2.4 4° passo: Sumarizar os dados numericamente Temos visto, até este ponto, como resumir os dados graficamente, de maeira geral. Porém muitas vezes precisamos demonstrar, numericamente, quais são os valores que resumem os nosso dados. Estes valores consistem nas medidas de tendência central e nas suas medidas de dispersão. O R permite, de maneira simples, sumarizar os dados por meio da função summary(). summary(dados) ## ano local comprimento peso ## 2020:20 Ambiente 1:15 Min. :10.00 Min. :18.00 ## 2021:20 Ambiente 2:25 1st Qu.:12.00 1st Qu.:20.00 ## Median :14.00 Median :21.50 ## Mean :13.97 Mean :21.73 ## 3rd Qu.:15.00 3rd Qu.:23.00 ## Max. :19.00 Max. :28.00 summary(abundancia) ## mes N desvio ## 1 :1 Min. : 30.00 Min. : 8.00 ## 2 :1 1st Qu.: 47.50 1st Qu.: 9.75 ## 3 :1 Median : 80.00 Median :11.50 ## 4 :1 Mean : 88.75 Mean :11.75 ## 5 :1 3rd Qu.:120.00 3rd Qu.:14.00 ## 6 :1 Max. :180.00 Max. :17.00 ## (Other):6 Repare que ao aplicarmos esta função o R nos informou as variáveis presentes nestas planilhas e quando fator indicou quais são e quantos de cada existem. Quando númerico, nos retornou valores como média, mediana (segundo quartil), mínimo, máximo, primeiro e terceiro quartis. As medidas de tendência central mais conhecidas e aplicadas são média, mediana e moda. Para obtenção da média precisamos somar todos os valores e dividir pela quantidade de valores ou, no R, podemos fazer isso de maneira mais simples por meio da função mean(): mean(dados$comprimento) ## [1] 13.975 Como podemos verificar a média do comprimento na planilha dados é de 13,97510. De maneira similar podemos calcular a mediana, que consiste no valor central da distribuição de valores de uma dada variável. Podemos visualiza-la por meio da função median(). median(dados$comprimento) ## [1] 14 Percebe-se que o valor que se encontra no meio da distribuição da variável comprimento é 14. Até o momento aprendemos a explorar e sumarizar nossos dados graficamente. Visualizamos também a função summary() aplicada aos dados, que nos indica as medidas de tendência central média e mediana assim como outras características das variáveis como os quartis, mínimo e máximo. Essas medidas apresentadas tem como função indicar as características gerais das variáveis com as quais você está trabalhando. Porém há outras medidas que explicam os dados que são constantemente requeridas. Medidas de dispersão indicam a propagação dos dados em torno do valor central, ou seja, são utilizadas para demonstrar a variabilidade dos dados. Quando as medidas se concentrarem em torno do centro de distribuição dos dados (baixa dispersão), quer dizer que possuem baixa variabilidade. Quando os dados apresentam uma distribuição mais ampla em torno do seu valor central (alta dispersão), significa que apresentam uma alta variabilidade. Os limites inferior e superior de uma dada variável quantitativa são denominados mínimo e máximo e podem ser obtidos por meio das funções que calculam o mínimo e máximo. Apesar de seu resultado aparecer na função summary(), que aplicamos anteriormente, podemos obtê-la separadamente para uma variável especifica. Para isso aplicamos as funções min() e/ou max(). min(dados$comprimento) ## [1] 10 max(dados$comprimento) ## [1] 19 Uma outra forma de obter os valores máximo e mínimo é usando a função range(). range(dados$comprimento) ## [1] 10 19 Novamente repare que usamos o operador cifrão ($) após o nome do conjunto de dados. Como você aprendeu anteriormente ele permite acessar as variáveis que estão dentro do objeto dados. Com os valores mínimo e máximo podemos obter nossa primeira medida de dispersão dos dados. A amplitude que consiste na diferença entre seus valores. Esta é a forma mais simples de verificar a dispersão de seus dados. A vantagem dela é fornecer a distância na qual seus dados variam mantendo-se a mesma unidade de medida. A amplitude é utilizada para dados quantitativos, descrever a variabilidade de uma amostra, porém não para inferência estatística. Podemos calcula-la simplesmente utilizando a substração das funções que nos retornam seus valores. Veja o exemplo abaixo. max(dados$comprimento) - min(dados$comprimento) ## [1] 9 Uma outra forma de realizar esta mesma operação é utilizando a função diff() que calcula a diferença. Quando inserimos a função range() dentro da função diff() o R nos retorna o resultado da diferença dos valores obtidos pela função range(). Veja o exemplo abaixo. diff(range(dados$comprimento)) ## [1] 9 O desvio padrão é uma das medidas de variação mais importantes que iremos realizar. Ele mede a distância dos valores observados em relação a sua média. Durante a coleta de dados em estudos ambientais, o que normalmente é extraído de informação é uma fração de um todo, por exemplo, uma amostra de uma população. Portanto quando se deseja avaliar o desvio padrão, assim como outras medidas de variabilidade, devemos considerar se estamos avaliando uma amostra ou toda a população. Se uma amostra é considerada, nos referimos a ela como desvio padrão amostral e a representamos por (s) se toda a população é considerada nos referimos a ela como desvio padrão populacional e a representamos por (\\(\\sigma\\)). Neste livro vamos abordar e considerar o desvio padrão amostral. Para calcular o desvio padrão utilizamos a seguinte fórmula: \\[s = \\sqrt{\\sum^{N}_{i = 1}\\frac{(x_{i}-\\overline{x})²}{N-1}}\\] Onde, \\(\\sum^{N}_{i=1}\\) indica o somatório da expressão \\(\\frac{(x_{i}-\\overline{x})²}{N-1}\\) para \\(i\\) variando de 1 até o número total de elementos \\(N\\). \\(x_{i}\\) indica o valor medido da variável \\(x\\), \\(\\overline{x}\\) indica a média da variável \\(x\\) e \\(N\\) indica o número total de elementos da variável \\(x\\). Podemos calcular o desvio padrão a partir de suas partes, considerando sua fórmula ou a partir de uma função pré-existente no R. Vamos conferir as duas formas. Para facilitar o entendimento do código vamos começar criando os seguintes objetos: comp, com.media e N. comp &lt;- dados$comprimento comp.media &lt;- mean(dados$comprimento) N &lt;- length(dados$comprimento) comp ## [1] 10 11 15 12 17 14 19 16 15 12 14 15 12 12 14 13 12 11 14 14 13 12 11 15 12 ## [26] 14 13 12 11 15 15 17 16 17 15 16 15 16 15 17 comp.media ## [1] 13.975 N ## [1] 40 comp: contêm os dados da variável comprimento presente na planilha dados comp.media: contêm um dado que consiste na média da variável comprimento N: representa a quantidade de observações presentes na variável comprimento. Repare que aqui utilizamos uma nova função chamada length() a qual contabiliza o número de observações de objeto. Agora vamos realizar o desvio padrão de 3 formass. Onde na primeira vamos entender como lemos uma função. O ideal é ler de dentro para fora. Como uma equação matemática. Mas não se assuste é mais fácil do que parece. Nas outras 2 maneiras vamos aplicar uma função pre-existente no R. sqrt((sum((comp - comp.media)^2)) / (N - 1)) ## [1] 2.106005 sd(dados$comprimento) ## [1] 2.106005 sd(comp) ## [1] 2.106005 A leitura da primeira dá-se da seguinte forma: (comp - comp.media): Realize a diferença dos valores presentes em comp de sua média. ((comp - comp.media)^2): Eleve esses valores, obtidos da diferença, ao quadrado. (sum((comp - comp.media)^2)): Some todos os valores, obtidos pela diferença elevados ao quadrado. (sum((comp - comp.media)^2)) / (N - 1): Divide a soma dos valores da diferença elevado ao quadrado pelo número de observações menos 1. sqrt((sum((comp - comp.media)^2)) / (N - 1)): Extraia a raíz quadrada da divisão da soma dos valores da diferença elevado ao quadrado pelo número de observações menos 1. Repare que a leitura segue a ordem dos passos do cálculo que seria feito a mão usando a equação apresentada acima. Mas caso não queira escrever a fórmula, o R nos fornece a função sd() que pode ser aplicada a uma variável dentro de uma planilha de dados (segunda forma apresentada) ou a um vetor que contenha um conjunto de observações do seu interesse (terceira forma apresentada). Pronto, agora entendemos como ler um código que apresenta funções dentro de funções e entendemos como realizar o desvio padrão de três formas distintas (existem diversas outras formas) no R. Agora você já pode escolher a que mais lhe agrada. Qual você prefere? Bem, essa é uma escolha sua. Assim como o desvio padrão temos também a variância. Na variância, calculamos o grau de dispersão dos nossos dados em relação à média desses dados. Há todo um conjunto especial de análise de dados voltado para ela. Mas como poderão ver, ela não difere muito do desvio padrão, apenas pelo fato de que consiste no seu valor elevado ao quadrado, daí a variância amostral ser representada por s² enquanto que o desvio padrão é representado por s. Partindo dessa ideia, a variância consiste em estimar o desvio padrão e eleva-lo ao quadrado (ou removermos a raíz quadrada da fórmula do desvio padrão). Vamos observar o seu resultado utilizando sua fórmula e a função predefinida pelo R. (sum((comp - comp.media)^2)) / (N - 1) ## [1] 4.435256 sd(dados$comprimento)^2 ## [1] 4.435256 var(dados$comprimento) ## [1] 4.435256 Pronto, aqui estão 3 formas de encontrar a variância de seus dados. A primeira é similar ao que realizamos anteriormente para o desvio padrão, porém sem a raíz quadrada. A segunda consiste em elevar ao quadrado o resultado do desvio padrão e a terceira é aplicando a função var() à variável comprimento presente na planilha dados. Difícil? Espero que não. O coeficiente de variação é outra medida importante e de fácil interpretação sobre a variabilidade dos dados. Este consiste em representar a variação em torno da média em termos percentuais. Este é simples de calcular e intuitivo de interpretar. Não há, até o momento, uma função no pacote básico do R que o calcule. Vamos calcula-lo a partir de sua fórmula matemática e ver o que ele quer dizer. sd(dados$comprimento)/mean(dados$comprimento)*100 ## [1] 15.0698 Pronto, calculamos o coeficiente de variação da variável comprimento e obtivemos uma variação em torno da média de ±15,07%. Com isso verificamos as principais medidas de tendência central e dispersão. Mas fica, aqui, uma questão. Como incorpora-las aos gráficos para que assim possamos resumir os dados graficamente de maneira mais completa? Vamos utilizar nossa planilha dados e verificar a média do comprimento e do peso por ano e por ambiente e incorporar em cada um dos gráficos seus respectivos desvios padrões. Neste caso vamos primeiro precisar organizar os dados que queremos plotar. Como desejamos plotar a média do comprimento por ano e ambiente com seus respectivos desvios padrões, vamos primeiro construir passo a passo uma tabela que apresente 4 colunas. A primeira com o ano, a segunda com ambiente, a terceira com a média do comprimento e a última com o desvio padrão. Para isso vamos utilizar a função split(), que fragmenta um data frame em função de algum fator. Neste primeiro momento, vamos fragmentar o objeto dados em função do ano e alocar esses dois novos conjuntos de dados em um novo objeto chamado dados.2. Posteriormente vamos criar um objeto chamado ano.2020 onde vamos alocar o primeiro objeto (referente ao ano de 2020) que está dentro de dados.2 e um segundo objeto chamado ano.2021 onde vamos alocar o segundo objeto (referente ao ano de 2021) que está dentro de dados.2. Veja os comandos abaixo. dados.2 &lt;- split(x = dados, f = dados$ano) ano.2020 &lt;- dados.2$`2020` ano.2021 &lt;- dados.2$`2021` Repare que foram criados no environment três novos objetos: dados.2 o qual consiste em um objeto da classe lista que contêm o objeto dados separado por ano; ano.2020 que consiste no objeto da classe data frame (mesma classe do objeto dados) que contêm as informações referentes, somente ao ano de 2020; e o objeto ano.2021, similar ao anterior, porém com as informações referentes ao ano de 2021. A seguir vamos aplicar o mesmo racíocinio lógico para fragmentar os objetos ano.2020 e ano.2021 por locais. ano.2020.local &lt;- split(x = ano.2020, f = ano.2020$local) ano.2021.local &lt;- split(x = ano.2021, f = ano.2021$local) ano.2020.local.1 &lt;- ano.2020.local$`Ambiente 1` ano.2020.local.2 &lt;- ano.2020.local$`Ambiente 2` ano.2021.local.1 &lt;- ano.2021.local$`Ambiente 1` ano.2021.local.2 &lt;- ano.2021.local$`Ambiente 2` Pronto! Dividimos nossos dados de acordo com os fatores que desejamos (ano e local) utilizando o argumento “f” da função split(). Agora vamos obter as métricas média e desvio padrão para cada ano de cada local. Para isso vamos criar novos objetos que alocam esses dados. ano.2020.local.1.comp.media &lt;- mean(ano.2020.local.1$comprimento) ano.2020.local.1.comp.dp &lt;- sd(ano.2020.local.1$comprimento) ano.2020.local.2.comp.media &lt;- mean(ano.2020.local.2$comprimento) ano.2020.local.2.comp.dp &lt;- sd(ano.2020.local.2$comprimento) ano.2021.local.1.comp.media &lt;- mean(ano.2021.local.1$comprimento) ano.2021.local.1.comp.dp &lt;- sd(ano.2021.local.1$comprimento) ano.2021.local.2.comp.media &lt;- mean(ano.2021.local.2$comprimento) ano.2021.local.2.comp.dp &lt;- sd(ano.2021.local.2$comprimento) Pronto! Média e desvio padrão do comprimento calculado para cada ano e local e alocados em seus respectivos objetos. Agora vamos unir esses resultados, por meio da função c() em um objeto com os dados referentes a média dados.media e um outro com o desvio padrão dados.dp. dados.media &lt;- c(ano.2020.local.1.comp.media, ano.2020.local.2.comp.media, ano.2021.local.1.comp.media, ano.2021.local.2.comp.media) dados.dp &lt;- c(ano.2020.local.1.comp.dp, ano.2020.local.2.comp.dp, ano.2021.local.1.comp.dp, ano.2021.local.2.comp.dp) Agora, vamos criar as variáveis categóricas ano e local para adicionar aos dados de médias e desvios e unir esses objetos em um data frame chamado dados.3. Posteriormente vamos conferir as classes das variáveis. Para isso durante este processo utilizaremos duas funções que não haviamos utilizado antes. As funções as.data.frame() que faz com que um conjunto de objetos sejam convertidos em data frame e cbind() o qual une objetos como colunas. Essas funções também serão comumente utilizadas durante seus passos iniciais no R. ano &lt;- c(&quot;2020&quot;, &quot;2020&quot;, &quot;2021&quot;, &quot;2021&quot;) local &lt;- c(&quot;Ambiente 1&quot;, &quot;Ambiente 2&quot;, &quot;Ambiente 1&quot;, &quot;Ambiente 2&quot;) dados.3 &lt;- as.data.frame(cbind(ano, local, dados.media, dados.dp)) str(dados.3) ## &#39;data.frame&#39;: 4 obs. of 4 variables: ## $ ano : chr &quot;2020&quot; &quot;2020&quot; &quot;2021&quot; &quot;2021&quot; ## $ local : chr &quot;Ambiente 1&quot; &quot;Ambiente 2&quot; &quot;Ambiente 1&quot; &quot;Ambiente 2&quot; ## $ dados.media: chr &quot;14.1&quot; &quot;13.1&quot; &quot;13&quot; &quot;14.8&quot; ## $ dados.dp : chr &quot;2.84604989415154&quot; &quot;1.28668393770792&quot; &quot;1.58113883008419&quot; &quot;1.93464651625488&quot; dados.3$ano &lt;- as.factor(dados.3$ano) dados.3$local &lt;- as.factor(dados.3$local) dados.3$dados.media &lt;- as.numeric(dados.3$dados.media) dados.3$dados.dp &lt;- as.numeric(dados.3$dados.dp) Vejamos nosso resultado final executando o nome do nosso objeto dados.3. dados.3 ## ano local dados.media dados.dp ## 1 2020 Ambiente 1 14.1 2.846050 ## 2 2020 Ambiente 2 13.1 1.286684 ## 3 2021 Ambiente 1 13.0 1.581139 ## 4 2021 Ambiente 2 14.8 1.934647 Pronto! depois de alguns (muitos) passos conseguimos construir nossa nova tabela com os dados resumidos. É um processo longo porém necessário para se acostumar com o ambiente de trabalho e a linguagem R. Há caminhos mais rápidos e tão eficientes quanto. Que se tornam úteis quando lidamos com muitas categorias dentro de uma variável. Contudo para isso é necessário a instalação de algum pacote (Há vários pacotes que apresentam funções que permitem a realização deste processo de maneira mais rápida e eficiente). Vamos utilizar um exemplo começando pela instalação do pacote chamado Rmisc. O comando abaixo realiza a instalação desse pacote. Conforme já fizemos com o pacote readxl. install.packages(&quot;Rmisc&quot;) Após instalado vamos carrega-lo pelo comando a seguir. Conforme já fizemos com o pacote readxl. library(Rmisc) Agora vamos conhecer a função summarySE() deste pacote o qual realiza os passos que fizemos anteriormente e ainda apresenta outros resultados. Mas iremos focar na média e desvio padrão. resultado &lt;- summarySE(data = dados, measurevar = &quot;comprimento&quot;, groupvars = c(&quot;ano&quot;, &quot;local&quot;)) resultado ## ano local N comprimento sd se ci ## 1 2020 Ambiente 1 10 14.1 2.846050 0.9000000 2.0359414 ## 2 2020 Ambiente 2 10 13.1 1.286684 0.4068852 0.9204382 ## 3 2021 Ambiente 1 5 13.0 1.581139 0.7071068 1.9632432 ## 4 2021 Ambiente 2 15 14.8 1.934647 0.4995236 1.0713715 Repare que a função summarySE() apresenta, basicamente, 3 argumentos, “data” que pede que se insira a planilha a ser utilizada, “measurevar” o qual pede a variável quantitativa que será sumarizada e “groupvars” o qual pede as variáveis categóricas11 segundo as quais a variável mensurável será agrupada. Observe também que para esta função as variáveis devem ser colocadas entre aspas. Pronto! Com a função summarySE() realizamos os mesmos passos que antes e guardamos o resultado dentro de um objeto chamado resultado. Repare que essa função nos resumiu os dados de comprimento de acordo com as duas variáveis categóricas que indicamos: ano e local. Em seu resumo ele nos forneceu, na seguinte ordem, as colunas: ano, local, N (número de observações), comprimento (média do comprimento), sd (desvio padrão), se (erro padrão) e ci (intervalo de confiança de 95%). OBS: Este é um exemplo sobre a vantagem e importância de usar funções que estão presentes em outros pacotes. Agora vamos usar o objeto resultado para desenvolver o gráfico de barras com o erro padrão (Figura 2.17). Porém para este caso, teremos que guardar nosso gráfico dentro de algum objeto, neste caso chamamos o objeto de grafico. Após o gráfico ser criado devemos utilizar a função arrows() que nos permite adicionar as barras de desvio padrão no nosso gráfico. grafico &lt;- barplot(formula = comprimento ~ ano + local, data = resultado, beside = TRUE, col = c(&quot;red&quot;, &quot;blue&quot;), ylim = c(0, 30), ylab = &quot;Comprimento médio (cm)&quot;, legend.text = TRUE) arrows(x0 = grafico, y0 = resultado$comprimento + resultado$sd, y1 = resultado$comprimento - resultado$sd, code = 3, angle = 90, length = 0.05) Figura 2.17: Gráfico de barras com erro padrão do comprimento por ano e local, da planilha resultado (derivada da planilha dados) Note que na função barplot() utilizamos algumas mudanças em relação ao que fizemos anteriormente, como os argumentos “formula”12 o qual indica a variável resposta em função das variáveis categóricas o qual será plotado (caso altere a ordem ano + local para local + ano a ordem do gráfico também será alterado. Teste você mesmo, veja o acontece). Neste caso, como temos mais de uma variável categórica adicionamos um argumento novo chamado “legend.text” o qual adiciona uma legenda ao gráfico. A função arrows() que adicionou as barras de desvios no gráfico apresenta alguns argumentos diferentes do que já vimos em outros gráficos. x0: o qual indica o objeto onde o gráfico foi armazenado; y0: o qual é escrito como uma operação matemática onde devemos incluir o valor da média + coluna onde está a variável referente ao desvio; y1: o qual é escrito como uma operação matemática onde devemos incluir o valor da média - coluna onde está a variável referente ao desvio; code: determina o tipo de seta que será plotada; angle: é o ângulo da haste da barra até a ponta; length: é o comprimento das bordas da barra; Terminamos de construir e entender os componentes do nosso primeiro gráfico com suas barras de erro. O principal para construção desse gráfico consiste em entender como organizar os dados a serem graficados. Ao se usar a função barplot() para construção do gráfico de barras com os desvios, devemos ter uma ou mais colunas com as categorias, outra com os valores que desejamos e outra com os valores dos desvios. Vamos realizar um segundo tipo de gráfico onde temos apenas uma variável categórica e seus respectivos desvios. Para isso utilizaremos nosso conjunto de dados abundancia (lembre-se que mes é uma variável categórica e deve ser identificada como fator) (Figura 2.18). Neste conjunto de dados não precisamos reorganiza-los pois já se encontram organizados e já temos os valores dos desvios. grafico &lt;- barplot(abundancia$N, ylim = c(0, 200), xlab = &quot;Mês&quot;, ylab = &quot;Abundância&quot;, names.arg = abundancia$mes, col = &quot;green&quot;) arrows(x0 = grafico, y0 = abundancia$N + abundancia$desvio, y1 = abundancia$N - abundancia$desvio, code = 3, angle = 90, length = 0.05) Figura 2.18: Gráfico de barras com erro padrão da abundância por mês da planilha abundância Repare que é a mesma coisa que fizemos anteriormente,porém com outro conjunto de dados. 2.5 Considerações Ao longo dos tópicos abordados neste capítulo demonstramos as etapas mais importantes e de diversos modos para nossa trajetória inicial e necessária no R, desde a definição do ambiente de trabalho, passando pela importação da planilha de dados, verificação e ajuste dos dados e sua sumarização gráfica e numérica (Figura 2.19). Durante nosso percurso neste livro realizaremos mais gráficos aliados as análises estatísticas básicas comumente utilizadas. Não se assuste com a quantidade de etapas, funções, argumentos e nomes, eles irão se repetir tantas e tantas vezes daqui em diante que você não esquecerá. O aprendizado do R é exponencial e ele lhe permitirá entender como nunca o universo da análise de dados adequando-o para seu campo de atuação. Figura 2.19: Resumo dos passos abordados no capítulo: da definição do ambiente de trabalho até a sumarização dos dados. 2.6 Exercícios Para iniciarmos nosso trabalho no R precisamos saber importar os dados com os quais iremos trabalhar. Porém alguns passos são fundamentais antes da importação. Qual das opções a seguir contêm as funções, em ordem, que contemplam esses passos. setwd(); choose.dir(); as.factor() e readxl() setwd(); choose.dir(); readxl() library(); setwd(); choose.dir() choose.dir(); setwd(); library() choose.dir(); setwd() Ao importarmos nossos dados é importante saber a extensão em que os arquivos são encontrados (ex: *.csv ou *.xlsx). A função para importação dos dados é determinada pela extensão dos arquivos em que os dados se encontram. Qual as opções abaixo melhor apresenta os comandos responsáveis por importar um arquivo em *.csv e outro em *.xlsx? read_csv(file = “dados.csv”, header = TRUE, sep = “,”); read.excel(path = “dados.xlsx”, col_names = TRUE) read_csv(file = “planilha.csv”, header = TRUE, sep = “;”); read_excel(path = “planilha.xlsx”, col_names = TRUE) read.csv(path = “planilha.csv”, header = TRUE, sep = “,”); read_excel(file = “planilha.xlsx”, col_names = TRUE) read.csv(file = “dados.csv”, header = TRUE, sep = “;”); read_excel(path = “planilha.xlsx”, col_names = TRUE) read.csv(file = “planilha.csv”, header = TRUE, sep = “,”); read.excel(path = “planilha.xlsx”, col_names = TRUE) Durante o processo de importação dos dados é necessário checar se todas as variáveis foram entendidas pelo R como elas devem ser (ex. se determinada variável foi identificada como fator ao invés de numérico ou caracter). Para isso algumas funções são importantes para checar e alterar se necessário. Qual das opções abaixo indica as funções que permitem checar e alterar as variáveis, se necessário. checar: View(), summary(); alterar: as.factor(), as.numeric(), as.data.frame() checar: View(), str(); alterar: as.factor(), as.numeric(), as.integer() checar: str(), as.data.frame(); alterar: as.data.frame(), as.numeric(), as.caracter() checar: str(), summary(); alterar: as.caracter(), as.factor(), as.numeric() checar: View(), as.data.frame(); alterar: as.integer(), as.numeric(), as.factor() O R apresenta em alguns de seus pacotes conjuntos de dados que nos permitem fazer análises. Usando o conjunto de dados chamado iris, o qual pode ser carregado usando a função data(). Qual das opções abaixo apresenta o comando para gerar o gráfico apresentado? data(iris) boxplot(Petal.Length ~ Species, data = iris, col = c(&quot;blue&quot;, &quot;red&quot;, &quot;yellow&quot;), xlab = &quot;Espécies&quot;, ylab = &quot;Comprimento da pétala&quot;, main = &quot;Meu Gráfico&quot;) boxplot(Petal.Length, Species, data = iris, col = c(“blue”, “red”, “yellow”), xlab = “Espećies”, ylab = “Comprimento da pétala”, main = “Meu Gráfico”) boxplot(Petal.Length ~ Species, data = iris, col = c(“blue”, “red”, “yellow”), xlab = “Espécies”, ylab = “Comprimento da pétala”, main = “Meu Gráfico”) boxplot(“Petal.Length” ~ “Species”, data = “iris”, col = c(“blue”, “red”, “yellow”), xlab = “Espécies”, ylab = “Comprimento da pétala”, main = “Meu Gráfico”) boxplot(Petal.Length ~ Species, data = iris, col = c(blue, red, yellow), xlab = “Espécies”, ylab = “Comprimento da pétala”, main = “Meu Gráfico”) boxplot(Petal.Length ~ Species, data = iris, col = c(“azul”, “vermelho”, “amarelo”), xlab = “Espécies”, ylab = “Comprimento da pétala”, main = “Meu Gráfico”) Execute os seguintes comandos no seu R: set.seed(458); teste &lt;- rnorm(100, sd = 0.2). Qual das opções abaixo representam, na ordem, a média, mediana, desvio padrão, variância e amplitude destes dados. 0,01; 0,02; 0,12; 0,04; 1,24 0,02; 0,01; 0,21; 1,24; 0,04 0,02; 0,01; 0,21; 0,04; 1,24 0,01; 0,02; 0,12; 0,04; 1,24 0,02; 0,01; 0,31; 0,04; 1,20 Ver figura 1.6↩ Tudo que é realizado no R é por meio de comandos, esses comandos são compostos de funções, podemos reconhece-las pois logo após um nome que representa uma função obrigatoriamente há um parênteses e cada função apresenta argumentos, depois dos quais há o sinal de igual (=). As funções estão dentro de pacotes/bibliotecas, ou seja, cada biblioteca apresenta diversas funções e cada função diversos argumentos. O R, quando instalado, traz consigo alguns pacotes, mas estes pacotes não apresentam uma função que permite carregar arquivos *.xls ou *.xlsx. Portanto para que seja possível realizar essa tarefa devemos instalar, neste caso, o pacote readxl. Para instalação de pacotes podemos usar a função conhecida por install.packages(“nome do pacote”).↩ tibble e data frame são, em termos práticos para o livro, objetos similares↩ Aqui estamos representando como vírgula o separador decimal. Mas o R em seu resultado utiliza o ponto (.) como separador decimal. Pois por padrão ele está configurado para o sistema númerico inglês↩ Repare que usamos a função concatenar c() para agrupar as 2 variáveis categóricas de interesse.↩ similar a função boxplot.↩ "],["qui-quadrado-chi.html", "Capítulo 3 Qui-quadrado (\\(\\chi\\)²) 3.1 \\(\\chi\\)² para ajuste de frequências 3.2 \\(\\chi\\)² para independência 3.3 Considerações 3.4 Exercícios", " Capítulo 3 Qui-quadrado (\\(\\chi\\)²) Até o momento aprendemos a definir o ambiente em que iremos trabalhar, importar nossa planilha, verifica-la e ajusta-la se necessário, assim como as métricas mais utilizadas para sumarizar os dados. Mas por vezes estamos interessados em analisar os dados, então vamos começar com o Qui-quadrado. Em alguns momentos podemos estar trabalhando com variáveis categóricas e lidarmos com perguntas sobre estas variáveis e portanto pode ser necessário avaliarmos se a frequência das observações de uma dada variável categórica ajusta-se ao que se é esperado (\\(\\chi\\)² para ajuste de frequências) ou se as proporções das observações de duas ou mais variáveis categóricas são independentes uma da outra (\\(\\chi\\)² para independência). De modo que, para este teste iremos precisar apenas de uma variável (se estivermos avaliando o ajuste de frequências) ou duas variáveis (se avaliarmos a independência) e estas devem ser categóricas. Como aprendemos anteriormente as variáveis categóricas devem ser classificadas como fator (pela função as.factor()) no R. Vamos conferir em detalhes e com exemplos como realizar estas duas análises no R. 3.1 \\(\\chi\\)² para ajuste de frequências Tabela 3.1: Principais características do qui-quadrado para ajuste das frequências Atributos Características Tipo de variável Categórica Quantidade de variáveis 1 Hipótese nula O número de observações em cada grupo da variável é similar ao predito pela teoria Fórmula \\[\\chi^2=\\sum^{N}_{i = 1}\\frac{(Ob-Es)^2}{Es}\\] Observação Se há apenas 2 categorias, dentro da variável, não há a necessidade de post-hoc nem expressa-la graficamente Imagine o seguinte exemplo: Você foi a praia e verificou uma enorme quantidade de produtos plásticos, esquecidos pelas pessoas. Após um tempo de observação você percebeu que haviam tanto sacolas quanto tampas de garrafa PET e poderia levantr a seguinte pergunta: Será que se coletarmos estes plásticos na praia e o categorizarmos eles apresentarão quantidades similares? A partir desta pergunta a seguinte hipótese nula (H0) é naturalmente construída: Não há diferenças entre estes dois tipos de plásticos (tampas e sacolas). O \\(\\chi\\)² objetiva responder a essa pergunta e fornecer subsídio estatístico para corroborar ou refutar essa hipótese. Após a observação, coleta, classificação e contagem dos fragmentos plásticos os seguintes valores foram observados: 520 sacolas plásticas e 470 tampas. Dessa forma podemos perguntar: do total de fragmentos coletados (520 para sacolas e 470 para tampas) cada um deles contribuem com 50% (percentuais iguais) do total? Vamos começar criando 2 vetores13 onde o primeiro corresponde aos valores observados e o segundo as proporções esperadas para cada valor. observados &lt;- c(520, 470) esperados &lt;- c(0.5, 0.5) observados ## [1] 520 470 esperados ## [1] 0.5 0.5 Note que criamos os objetos “observados” e “esperados”. O primeiro corresponde ao que coletamos e o segundo as proporções que esperamos de ambos. Para realizar o teste do \\(\\chi\\)² para ajuste de frequências, vamos utilizar a função chisq.test(). chisq.test(x = observados, p = esperados) ## ## Chi-squared test for given probabilities ## ## data: observados ## X-squared = 2.5253, df = 1, p-value = 0.112 Esta função utiliza 2 argumentos: * x: no qual inserimos o vetor númerico que contém os valores referentes a quantidade da variável nominal que contabilizamos; * p: no qual inserimos o vetor númerico que contem as proporções esperadas associado a cada grupo da categoria avaliada. Note que o resultado indica o teste realizado, o nome do objeto que contém os dados e na terceira linha ele apresenta o resultado como valor do teste do \\(\\chi\\)², o grau de liberdade (df do inglês degrees of freedom) e o valor de probabilidade associado (do inglês p-value). Uma característica do \\(\\chi\\)² é que quanto mais similares forem os valores observados menor será o valor do \\(\\chi\\)², mas nunca menor que 0 e com o p-value máximo igual a 1 e quanto mais dissimilares forem os valores observados maior será o \\(\\chi\\)² (tendendo a infinito) e menor será o p-value (tendendo a 0) Com isso podemos observar que o \\(\\chi\\)² varia entre (0 e + infinito) enquanto que o p-value varia entre (0 e 1) com uma relação inversa entre eles. Quanto maior o \\(\\chi\\)² menor o p-value e quanto menor o \\(\\chi\\)² maior o p-value. Por convenção indicamos que um teste estatístico que apresenta p-value &lt; 0,05 indica diferenças significativas entre os valores observados. Dessa forma, assumindo essa convenção, podemos concluir que a quantidade de sacolas não difere da quantidade de tampas de garrafa PET. Portanto aceitamos a hipótese nula. Não é comum representar o resultado deste teste graficamente apenas como valor mas se mesmo assim quiser representa-lo utilize um gráfico de barras. Vamos verificar outros exemplos e avaliar suas hipóteses nulas. Imagine um segundo exemplo. 300 flores foram fotografadas e classificadas pela cor e ao final foi obtido um total de 170 flores vermelhas e 130 flores verdes. Ao ler alguns trabalhos e analisar os resultados observados para outra região foi levantada a hipótese de que 70% das flores seriam vermelhas e 30% seriam verdes. Com base nesses dados podemos perguntar: As flores fotografadas confirmam os resultados esperados? Vamos avaliar este caso, conforme fizemos acima. flores.observadas &lt;- c(170, 130) flores.esperadas &lt;- c(0.7, 0.3) Observe que igual nosso exemplo anterior criamos objetos flores.observadas e flores.esperadas. Note que a diferença está nos valores esperados, onde alteramos a proporção, que não é mais de 50% para cada, mas sim de 70% para um e 30% para outro, porém expressos em proporção (valores variam entre 0 e 1). Perceba, também, que os valores seguem a ordem em que foram inseridos, portanto o primeiro valor observado inserido corresponde ao primeiro valor esperado inserido e assim por diante. Ao conduzir a análise temos o seguinte resultado chisq.test(x = flores.observadas, p = flores.esperadas) ## ## Chi-squared test for given probabilities ## ## data: flores.observadas ## X-squared = 25.397, df = 1, p-value = 4.667e-07 Perceba que o valor do \\(\\chi\\)² aumentou em relação ao exemplo anterior e o p-value diminuiu para um valor bem próximo a 0. Este resultado indica que os valores observados de flores vermelhas e verdes não apresentam as proporções que se esperam de 70% e 30%. Portanto refutamos a hipótese nula de que elas se encontram nas proporções esperadas. Vamos seguir em frente e olhar para um outro exemplo. Suponha que foram coletados 2660 indivíduos de siri em um estuário. Eles foram classificados em relação ao sexo e foram obtidos 1380 fêmeas e 1280 machos. Espera-se que a razão sexual desta espécie seja de 1:1 (50% para cada sexo). Os indivíduos amostrados confirmam o que é esperado? siris.observados &lt;- c(1380, 1280) siris.esperados &lt;- c(0.5, 0.5) chisq.test(x = siris.observados, p = siris.esperados) ## ## Chi-squared test for given probabilities ## ## data: siris.observados ## X-squared = 3.7594, df = 1, p-value = 0.05251 Com este caso podemos observar que o valor do \\(\\chi\\)² indicou um p-value próximo a 0,05, indicando que temos de ter cuidado com as afirmações relativas a hipótese testada. Contudo, como o valor é &gt;0,5 podemos corroborar a hipotese nula e dizer que os valores observados para fêmeas e machos encontram-se dentro da proporção esperada de 1:1. E se ao invés de 2 grupos tivessemos 3 ou mais. Como realizariamos o \\(\\chi\\)²? Vamos ver com um exemplo. Imagine que você foi a uma floresta e classificou as árvores em altas, médias e baixas e contabilizou 55, 31 e 27 respectivamente. De acordo com a literatura é esperado que as proporções de árvores altas, médias e baixas seja de 1:1:1. Vamos analisar e verificar se podemos afirmar que o observado corrobora o esperado. arvores.observadas &lt;- c(55, 31, 27) arvores.esperadas &lt;- c(1/3, 1/3, 1/3) chisq.test(x = arvores.observadas, p = arvores.esperadas) ## ## Chi-squared test for given probabilities ## ## data: arvores.observadas ## X-squared = 12.177, df = 2, p-value = 0.002269 Neste exemplo podemos perceber que a análise é igual a todos os casos anteriores, a diferença é que adicionamos um grupo a mais e consequentemente um valor a mais. Devido a essa mudança tivemos que dividir as probabilidades (objeto arvores.esperadas) em 3 valores e de igual probabilidade, já que o hipotetizado consistia em proporções iguais para os três grupos definidos. De acordo com o resultado obtido podemos perceber que o que foi observado não corrobora o esperado. Vamos observar quais seriam as quantidades de árvores esperadas de acordo com as quantidades que observamos e as proporções que esperamos. Para isso precisamos primeiramente inserir o resultado em um objeto e a seguir acessar os componentes desse objeto pelo operador matemático $ (cifrão), seguido pelo objeto expected criado pela função chisq.test(). resultado &lt;- chisq.test(x = arvores.observadas, p = arvores.esperadas) resultado$expected ## [1] 37.66667 37.66667 37.66667 Os exemplos acima tratam de avaliar o \\(\\chi\\)² em variáveis que apresentam dois ou mais grupos. Quando o grau de liberdade é 1 (dois grupos) e o número de observações é considerado baixo é sugerido a aplicação da correção de Yates. Quando o número de observações é elevado ela não distorce o resultado. O risco de se usar o \\(\\chi\\)² sem a correção de Yates consiste em inflar o valor de \\(\\chi\\)² e portanto diminuir o p-value o que nos levaria ao risco de rejeitarmos a hipótese nula quando ela é verdadeira (erro tipo I). A função do \\(\\chi\\)², no pacote base do R, não realiza a correção de Yates para este teste, apenas para o próximo caso que iremos tratar (\\(\\chi\\)² para independência). Contudo ele oferece outro método para avaliação deste teste que consiste na simulação de Monte Carlo. Não entraremos em detalhes de como funciona este teste, mas o que alertamos é que se o número de observações utilizado é considerado baixo correções devem ser aplicadas. Embora os cálculos envolvidos para realização da correção de Yates não seja complicada sugerimos a leitura da literatura especializada. Vamos verificar um exemplo onde a correção de Monte Carlo, a qual é possível de ser realizada de maneira rápida e prática no R, é aplicada. Imagine que você foi em campo e contabilizou as espécies de cracas que identificou. Foram contabilizados 12 cracas da espécie A e 24 da espécie B. Trabalhos anteriores demonstraram que ambas as espécies encontram-se na proporção de 1:1. Aplicando a simulação de Monte Carlo o número de indivíduos observados de ambas as espécies corroboram o esperado? N.especies &lt;- c(12, 24) prop.especies &lt;- c(0.5, 0.5) chisq.test(x = N.especies, p = prop.especies, simulate.p.value = FALSE) ## ## Chi-squared test for given probabilities ## ## data: N.especies ## X-squared = 4, df = 1, p-value = 0.0455 chisq.test(x = N.especies, p = prop.especies, simulate.p.value = TRUE) ## ## Chi-squared test for given probabilities with simulated p-value (based ## on 2000 replicates) ## ## data: N.especies ## X-squared = 4, df = NA, p-value = 0.07146 Basta usarmos o argumento “simulate.p.value” e indicar “FALSE” quando não queremos a simulação de Monte Carlo, ou apenas omitirmos este argumento, ou usar este argumento e indicar “TRUE” o qual realiza a simulação de Monte Carlo. Verifique que com a aplicação desta simulação o p-value aumenta indicando que torna mais dificil rejeitarmos a hipótese nula. Após aplicarmos a correção na nossa análise podemos afirmar que não há diferença na quantidade de cracas observadas em relação as proporções esperadas, para ambas as espécies de cracas. Relembrando. Sempre tome cuidado com afirmações! Principalmente quando o número de observações é baixo. Mesmo que o teste estatístico indique diferenças, uma baixa quantidade de observações devem ser analisadas cuidadosamente, no entanto devemos aplicar correções matemáticas nestes casos. 3.2 \\(\\chi\\)² para independência Realizamos este teste quando objetivamos saber se a frequência dos grupos de uma variável é independente de outra variável. Diferente do primeiro caso de \\(\\chi\\)² que abordamos, este lida com duas variáveis categóricas ao invés de uma. Portanto é necessário o desenvolvimento de uma tabela de contigência. A tabela de contigência consiste em uma tabela com o número de observações referentes as duas variáveis selecionadas. Veremos alguns exemplos abaixo. Para o momento. Veja o sumário abaixo no qual detalhamos as principais características desta análise. Tabela 3.2: Principais características do qui-quadrado para independência Atributos Características Tipo de variável Categórica. Quantidade de variáveis 2 ou mais. Hipótese nula As proporções relativas de uma variável são independentes de uma segunda variável. Fórmula Para uma tabela 2 $$2 temos: \\[\\chi^{2} = \\frac{n(f_{11}f_{22}-f_{12}f_{21})^2}{R_{1}R_{2}C_{1}C_{2}}\\] Onde n é onúmero total de observações; \\(f_{11}\\) é o valor da primeira linha e primeira coluna; \\(f_{22}\\) é o valor da segunda linha e segunda coluna; \\(f_{12}\\) é o valor da primeira linha e da segunda coluna; \\(f_{21}\\) é o valor da segunda linha e primeira coluna; \\(R_{1}\\) é a soma da primeira linha; \\(R_{2}\\) é a soma da segunda linha; \\(C_{1}\\) é a soma da primeira coluna e \\(C_{2}\\) é a soma da segunda coluna. Observação Como característica intrinseca a este teste é necessário a construção da tabela de contigência. Imagine a seguinte situação: Insetos foram coletados, ao acaso, em uma região costeira. Todos os insetos foram avaliados quanto a espécie e a presença de parasitas. Os seguintes valores foram obtidos: 55 indivíduos da espécie A com parasita, 75 indivíduos da espécie A sem parasita, 90 indivíduos da espécie B com parasita e 110 indivíduos da espécie B sem parasita. Foi levantada a hipótese nula de que a quantidade de indivíduos infectados pelo parasita é o mesmo para ambas as espécies. Vamos começar criando uma matriz14 com esses valores, onde as linhas corresponderão as espécies, as colunas a presença/ausência do parasita e os seus elementos corresponderão a quantidade de indivíduos dada as condições (espécie e parasita). Para construir nossos dados utilizaremos a função matrix, com os argumentos “data”, onde adicionaremos os elementos que estarão presentes, “ncol” onde definiremos a quantidade de colunas e “byrow” onde definiremos que os valores presentes no argumento “data” serão inseridos em ordem por linha, neste caso a função começará preenchendo a primeira linha da primeira coluna, segunda linha da primeira coluna, primeira linha da segunda coluna e segunda linha da segunda coluna. Guardaremos nossa matriz criada em um objeto chamado dados. Utilizaremos as funções rownames() e colnames() a fim de definir os nomes das linhas e colunas da nossa matriz para melhor visualização. A seguir iremos visualizar o objeto dados. dados &lt;- matrix(data = c(55, 75, 90, 110), ncol = 2, byrow = TRUE) rownames(dados) &lt;- c(&quot;Espécie A&quot;, &quot;Espécie B&quot;) colnames(dados) &lt;- c(&quot;Com parasita&quot;, &quot;Sem parasita&quot;) dados ## Com parasita Sem parasita ## Espécie A 55 75 ## Espécie B 90 110 Pronto nossa tabela de contigência está criada. Vamos seguir para a análise chisq.test(x = dados) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: dados ## X-squared = 0.13543, df = 1, p-value = 0.7129 Ao executarmos a função chisq.test() utilizando apenas a matriz como objeto do argumento “x” ele nos retorna o \\(\\chi\\)² com a correção de Yates. O que é sugerido para quando estamos lidando com matrizes \\(2 \\times 2\\). No geral o resultado é similar ao que executamos no tópico anterior. Uma atenção deve ser dada ao grau de liberdade o qual indica o valor de 1. O cálculo do grau de liberdade consiste no número de linhas menos um multiplicado pelo número de colunas menos um. Como temos 2 linhas e 2 colunas o grau de liberdade será: \\((2-1)*(2-1) = 1*1 = 1\\). De acordo com o resultado obtido pelo teste não podemos refutar nossa hipótese nula. Portanto dizemos que a proporção da população infectada pelo parasita é a mesma nas duas espécies. Vejamos um segundo exemplo. Durante 2 anos camarões foram coletados, em um estuário, e todos eles foram sexados em machos e fêmeas. Objetivou-se com esses dados saber se há algum desvio da razão sexual entre os anos. Vamos começar desenvolvendo um conjunto de dados na forma de planilha, que é como seus dados devem estar planilhados e vamos ensina-lo a construir uma tabela de contingência a partir desses dados. Execute o código abaixo. Não se preocupe, no momento, com as funções novas utilizadas. A única função diferente é rep() que repete um determinado elemento n vezes. No código abaixo, estaremos repetindo o elemento “Ano 1” 150 vezes e o elemento “Ano 2” 225 vezes e concatenaremos estes elementos por meio da função c() e guardando-o no objeto ano faremos o mesmo para o objeto sexo onde concatenaremos a repetição do elemento “macho” 55 vezes, seguido por “fêmea” 95 vezes, seguido novamente por “macho” 105 vezes e por fim “fêmea” 120 vezes. Após isso uniremos os dois objetos ano e sexo por meio da função cbind() e diremos que isso consiste em um objeto da classe data frame por meio da função data.frame() e guardaremos esse resultado em um objeto denominado dados. ano &lt;- c(rep(&quot;Ano 1&quot;, 150), rep(&quot;Ano 2&quot;, 225)) sexo &lt;- c(rep(&quot;macho&quot;, 55), rep(&quot;fêmea&quot;, 95), rep(&quot;macho&quot;, 105), rep(&quot;fêmea&quot;, 120)) dados &lt;- data.frame(cbind(ano, sexo)) Vamos visualizar nossos dados. Para isso podemos usar a função head() que mostra no console as primeiras linhas desse data frame. head(dados) ## ano sexo ## 1 Ano 1 macho ## 2 Ano 1 macho ## 3 Ano 1 macho ## 4 Ano 1 macho ## 5 Ano 1 macho ## 6 Ano 1 macho Agora vamos, a partir de dados construir uma tabela de contigência. Para isso vamos utilizar a função table() e guardar seu resultado em um objeto chamado tab.cont. A seguir iremos visualizar nossa tabela de contingência chamando tab.cont tab.cont &lt;- table(dados) tab.cont ## sexo ## ano fêmea macho ## Ano 1 95 55 ## Ano 2 120 105 Vamos agora realizar o qui-quadrado de nossos dados. chisq.test(tab.cont) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: tab.cont ## X-squared = 3.2817, df = 1, p-value = 0.07006 Pronto! Como podem ver, a representação do resultado é similar ao exemplo acima. Contudo nossos dados são diferentes e portanto nossa hipótese nula é diferente. Nossa hipótese nula pode ser escrita de três formas. Vejamos os 3 exemplos abaixo: H0: Na população amostrada a proporção de machos e fêmeas é independente do ano; H0: Na população amostrada a razão sexual dos camarões é a mesma entre os anos; H0: A proporção de camarões entre os anos é a mesma para ambos os sexos; Como o teste resultou em um p-value maior que 0,05 podemos dizer, por essa via, que corroboramos nossa hipótese nula. Neste ponto é bom relembramos da importância do número de observações de cada grupo. Se este número for pequeno, incertezas podem ser geradas e ajustes devem ser feitos. Contudo, para definir o que é um número baixo de observações sigamos para o cálculo sugerido por Zar (2010)15 o qual define como: \\(n = 6rc\\). Onde, n é o número de observações mínimo de cada grupo, r é o número de linhas (rows) e c é o número de colunas (column). Dessa forma, em uma tabela de contigência do tipo \\(2 \\times 2\\) o número de observações mínimo para cada grupo deve ser de: \\(n = 6 \\times (2) \\times (2) = 24\\). Ou seja, 24 observações. Esta análise pode ser conduzida, também, quando lidamos com mais de um grupo em uma categoria e neste caso teriamos uma tabela de contigência \\(2 \\times 3\\), \\(2 \\times 4\\), \\(3 \\times 3\\) e assim por diante. Vamos ver um exemplo. Três pesquisadores analisaram fotografias de espécies bentônicas em um costão objetivando avaliar a abundância de duas espécies, uma introduzida e uma nativa. Considere todo o processo amostral similar e que os pesquisadores avaliaram as mesmas fotografias. Neste caso nossa hipótese nula é que a proporção entre as espécies é similar independentemente dos pesquisadores. Vamos construir nossa matriz de uma outra forma. pesquisadores &lt;- c(&quot;pesquisador 1&quot;, &quot;pesquisador 2&quot;, &quot;pesquisador 3&quot;) sp.nativa &lt;- c(70, 79, 38) sp.introduzida &lt;- c(84, 95, 80) dados &lt;- matrix(c(sp.nativa, sp.introduzida), nrow = 2, byrow = T) colnames(dados) &lt;- pesquisadores rownames(dados) &lt;- c(&quot;Espécie nativa&quot;, &quot;Espécie introduzida&quot;) dados ## pesquisador 1 pesquisador 2 pesquisador 3 ## Espécie nativa 70 79 38 ## Espécie introduzida 84 95 80 Os dados foram construídos passo a passo. Vamos analisar e verificar o resultado. chisq.test(dados) ## ## Pearson&#39;s Chi-squared test ## ## data: dados ## X-squared = 6.2322, df = 2, p-value = 0.04433 Repare que há algo diferente, como realizamos a análise em uma tabela \\(2 \\times 3\\) por padrão a função chisq.test() não realiza a correção de Yates. Pois não é necessário. Analisando o resultado podemos inferir que as proporções observadas de espécies nativas e introduzidas são diferentes em função do pesquisador. Ou seja, rejeitamos nossa hipótese nula de que as proporções são similares. Contudo o p-value foi bem próximo ao limiar de 0,05. Podemos nos aprofundar na questão e nos perguntar onde está essa diferença? Entre o pesquisador 1 e o pesquisador 2 ou entre outra dupla de pesquisadores? Como observamos isso? Para responder essa pergunta em específico, vamos subdividir nossa tabela de contigência para todos os pares de observações e armazena-los em outros objetos. Para isso criaremos tabelas \\(2 \\times 2\\) para cada par de comparações. Repare que usaremos a nossa tabela dados originalmente criada seguida por colchetes e dentro do colchetes inserimos virgula e menos o número referente a coluna. O colchetes após o nome dos dados nos permite acessar as linhas e colunas de um conjunto de dados quaisquer com isso podemos remove-las ou seleciona-las. Qualquer número antes da virgula indica a linha e qualquer número após a vírgula indica a coluna. O sinal de menos indica que vamos remover e se não colocarmos sinal indica que iremos selecionar o valor especificado. No primeiro caso onde inserimos “-3” após a virgula estamos indicando que vamos remover a terceira coluna do objeto dados e armazenar este resultado em um novo objeto chamado p1.p2. O mesmo raciocínio segue nos demais casos. p1.p2 &lt;- dados [, -3] p1.p3 &lt;- dados [, -2] p2.p3 &lt;- dados [, -1] Sabendo isso vamos realizar a função chisq.test() para cada par de observações e analisar seu resultado. chisq.test(p1.p2) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: p1.p2 ## X-squared = 0, df = 1, p-value = 1 chisq.test(p1.p3) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: p1.p3 ## X-squared = 4.3623, df = 1, p-value = 0.03674 chisq.test(p2.p3) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: p2.p3 ## X-squared = 4.5663, df = 1, p-value = 0.03261 Repare que não há diferenças entre a proporção de espécies observadas entre os pesquisadores 1 e 2. Mas entre o pesquisador 3 e os demais há diferença. Isto indica que é o pesquisador 3 que destoa dos demais em relação a observação de espécies nativas e introduzidas. Quando observamos todo o conjunto de dados não observamos esta característica, mas ao separarmos os dados a diferença fica evidenciada. Uma prática comum nesta análise é demonstrar o resultado por meio gráfico. Portanto, vamos observar como representar graficamente este último exemplo. Primeiro vamos transformar nossa tabela de contigência em uma tabela no qual os valores irão corresponder a proporções relativas. Para isso vamos usar a função prop.table() e armazenar este resultado em um objeto chamado tabela. tabela &lt;- prop.table(x = dados) tabela ## pesquisador 1 pesquisador 2 pesquisador 3 ## Espécie nativa 0.1569507 0.1771300 0.08520179 ## Espécie introduzida 0.1883408 0.2130045 0.17937220 Como podem ver o resultado deste nosso passo nos retornou uma tabela de proporções em relação ao total. Podemos transformar-la em percentual, basta multiplicarmos a tabela por 100. Vamos ver o processo tabela &lt;- prop.table(x = dados)*100 tabela ## pesquisador 1 pesquisador 2 pesquisador 3 ## Espécie nativa 15.69507 17.71300 8.520179 ## Espécie introduzida 18.83408 21.30045 17.937220 Pensando um pouco na nossa análise e no que ela avalia. O \\(\\chi\\)² está avaliando as proporções relativas entre os grupos de uma dada variável, neste último caso a proporção das espécie em relação aos pesquisadores. Então a tabela de proporções ou percentual que nos interessa é a relação entre a espécie nativa e introduzida por pesquisador. A função prop.table() nos permite definir isso por meio do argumento “margin”, vamos verificar o resultado olhando para o resultado em percentual. prop.table(x = dados, margin = 1)*100 ## pesquisador 1 pesquisador 2 pesquisador 3 ## Espécie nativa 37.43316 42.24599 20.32086 ## Espécie introduzida 32.43243 36.67954 30.88803 prop.table(x = dados, margin = 2)*100 ## pesquisador 1 pesquisador 2 pesquisador 3 ## Espécie nativa 45.45455 45.4023 32.20339 ## Espécie introduzida 54.54545 54.5977 67.79661 Como podem visualizar, se usarmos “margin = 1” ele nos retorna os valores em função das linhas (espécies), se utilizarmos “margin = 2” ele nos retorna os valores em função das colunas (pesquisadores). Vamos utilizar “margin = 2” para construir nossa tabela e plotarmos nosso gráfico (Figura 3.1). tabela &lt;- prop.table(x = dados, margin = 2)*100 par(mar = c(4, 4, 4, 13)) barplot(tabela, xlab = &quot;Percentual de espécies&quot;, col = c(&quot;red&quot;, &quot;darkblue&quot;), legend.text = TRUE, args.legend = list(x = &quot;right&quot;, bty = &quot;n&quot;, inset = -0.5)) Figura 3.1: Gráfico de barras sobrepostas indicando o percentual relativo da observação de espécies nativas (cor vermelha) e introduzidas (cor azul), por pesquisador. Como podem ver o nosso gráfico foi realizado. Mas adicionamos uma série de comandos novos. Vamos explica-los passo a passo. Primeiro contruímos a tabela com os dados em percentuais relativos ao pesquisador e o armazenamos em um objeto chamado tabela. Posteriormente utilizamos uma função chamada par() com o argumento “mar” no qual concatenamos 4 valores. Esta função juntamente com esse argumento permite que redimensionemos a janela gráfica. Os quatro valores concatenados referem-se as margens inferior, esquerda, superior e direita, respectivamente. Nosso último passo é a nossa já conhecida função barplot(), onde inserimos como primeiro valor o objeto que contem os dados que queremos graficar tabela, seguido pelo argumento “col” que define as cores de acordo com as linhas da tabela, por sua vez definimos se a legenda será plotada com o argumento “legend.text” e por último indicamos a posição e o formato da legenda por meio do argumento “args.lengend”. Neste último argumento precisamos inserir a função list() na qual indicamos a posição da legenda pelo argumento “x”, o argumento “bty” o qual define que a legenda não tera uma caixa desenhada no seu entorno e o argumento “inset” que define a posição da legenda em relação ao eixo x. 3.3 Considerações Durante nosso percurso neste capítulo realizamos etapas que consistem na construção dos dados no próprio programa do R e a condução da análise do qui-quadrado para ajuste das frequências e para a independência. A construção dos dados tem como resultado final um objeto similar a planilha de dados que deve ser importada, caso já a tenha preparado, e juntamente com a sumarização dos dados temos as mesmas etapas trabalhadas no capítulo anterior. A partir daí temos a condução da análise estatística pelo qui-quadrado. Um resumo gráfico das etapas pode ser observado aqui (Figura 3.2). Figura 3.2: Resumo dos passos abordados no capítulo: da preparação dos dados até a análise dos dados. 3.4 Exercícios Durante um longo período de observação sobre a trafego de carros na praia um pesquisador percebeu que durante a tarde o número de carros circulando era maior que durante a manhã. Ele resolveu tomar nota da quantidade de carros que passava em um determinado trecho da praia, durante vários dias, em ambos os períodos (manhã e tarde) e registrou, no total, os seguintes valores: 550 carros durante a manhã 620 carros durante a tarde. Considerando a observação do pesquisador e os valores por ele tomados, podemos corroborar ou refutar sua observação? Desenvolva a análise do qui-quadrado. Ao registrar o número de carros o pesquisador também registrou os dias da semana em que foram feitas as observações e obteve os seguintes valores: Manhã de Quarta-feira: 130 Manhã de Sexta-feira: 420 Tarde de Quarta-feira: 178 Tarde de Sexta-feira: 442 A partir desses registros podemos dizer que o número de carros por período do dia (manhã e tarde) é independente do dia da semana (Quarta-feira e Sexta-feira)? Considerando os dados do exercício anterior. Obtenha a tabela com os dados em percentuais do número de carros por período do dia em relação aos dias da semana. Defina erro do tipo 1 e erro do tipo 2. Considerando o exercício 2. Qual dos erros (tipo 1 ou tipo 2) é assumido a um nível de confiança de 95%? Plote um gráfico com a tabela obtida no exercício 3. Vetores é uma estrutura básica do R que consiste em um objeto que contêm elementos de mesma classe. Se realizarmos o comando c(1, 2, 3), criamos um vetor, mas não um objeto. Se criarmos o seguinte: teste &lt;- c(1, 2, 3) estaremos criando um objeto do tipo vetor.↩ É um conjunto de elementos com estrutura n-dimensional, ou seja, com várias linhas e colunas, onde todos os seus elementos são de mesmo tipo (númerico, inteiro, fator, caracter etc).↩ Zar, J. H. (2010). Biostatistical analysis (5th ed.). Upper Saddle River, N.J.: Prentice-Hall/Pearson.↩ "],["teste-t.html", "Capítulo 4 Teste-t 4.1 Teste-t para uma amostra 4.2 Teste-t para duas amostras 4.3 Teste-t pareado 4.4 Considerações 4.5 Exercícios", " Capítulo 4 Teste-t Ao iniciarmos nossos estudos geralmente estamos interessados em saber se alguma característica da população que coletamos e analisamos é similar a uma outra população. Podemos abordar essa ideia por diversos caminhos porém vamos iniciar essas análises em R através do teste-t para uma amostra e a discussão sobre hipótese nula, caudalidade e nível de confiança. A partir de então seguiremos para o teste-t com duas amostras e o teste-t pareado. 4.1 Teste-t para uma amostra Conduzimos esse teste quando objetivamos verificar se a média de uma dada variável é similar a um valor esperado. Ao falarmos de média percebemos que a variável com a qual estamos lidando é do tipo númerica ou inteiro (a diferença entre ambas consiste na presença de casas decimais, na variável númerica), ou seja, mensurável (= quantitativo). Ao compararmos a média de nossa variável a um valor previamente estipulado duas hipóteses são naturalmente construídas, a hipótese nula (H0) e a hipótese alternativa (HA) (Tabela 4.1). Tabela 4.1: Principais características do teste-t para uma amostra Atributos Características Tipo de variável Quantitativa Quantidade de variáveis 1 Hipótese nula A média da variável é similar ao valor previamente estipulado Fórmula \\[t=\\frac{(\\overline{X}-\\mu)}{(s/sqrt(N))}\\] Onde, \\(\\overline{X}\\): média da amostra, \\(\\mu\\): média teórica esperada, s: desvio padrão da amostra e N: tamanho da amostra. Observação Não há a necessidade de post-hoc nem expressa-la graficamente Caso você ainda não tenha seus dados, vamos verificar como conduzir o teste-t por meio de exemplos utilizando valores fictícios. Imagine o seguinte exemplo: 100 camarões foram coletados em um estuário. Suas medidas em relação ao tamanho foram tomadas e deseja-se saber se o seu tamanho médio é similar ou não ao tamanho médio (25,80mm) da mesma espécie, observada em outro estuário. A partir do exemplo acima podemos definir nossas hipóteses, onde. H0: média observada é igual à média esperada; HA: média observada é diferente da média esperada; Vamos começar gerando os dados relativos ao tamanho observado dos camarões que coletamos. Para isso vamos utilizar a função set.seed() que define os números aleatórios que serão gerados. Dessa forma, quando aplicado essa função associada a um número comum, no caso “1234”, os números aleatórios que você irá gerar para o objeto tamanho.camarao, por exemplo, serão os mesmos dos apresentados aqui. A nossa segunda linha de comando utilizará a função rnorm() o qual gera números aleatórios considerando uma distribuição normal e soma esses valores gerados a uma distribuição uniforme gerada pela função runif(). A estas funções adicionamos argumentos que definem a quantidade de números gerados (argumento “n”), o valor mínimo que pode ser gerado (argumento “min”) e o valor máximo que pode ser gerado (argumento “max”). E guardamos o resultado obtido em um objeto chamado tamanho.camarao. Relembre que o nome do objeto não deve apresentar espaços ou acentos, que pode gerar problemas e/ou dificuldades na condução das análises. set.seed(1234) tamanho.camarao &lt;- rnorm(n = 100) + runif(n = 100, min = 17, max = 33) Vamos verificar o resultado. Basta digitarmos o nome do objeto. tamanho.camarao ## [1] 26.36501 25.73118 23.16434 26.93999 25.85006 29.22289 21.34791 22.92014 ## [9] 19.70599 31.88009 25.58378 20.48762 19.18464 29.19344 27.02799 31.80449 ## [17] 26.70808 27.30077 23.83039 33.02083 23.89138 17.01159 20.68980 22.81711 ## [25] 18.44307 23.54454 30.40893 21.37080 25.12759 23.97507 30.85514 25.59575 ## [33] 17.99771 29.42112 24.44470 19.22824 26.81323 20.57450 24.53800 32.36946 ## [41] 25.23524 19.84181 19.61879 27.74526 31.68906 23.66401 28.26906 24.93702 ## [49] 31.93121 29.25393 23.70445 25.96390 20.11329 20.45772 17.87934 26.57236 ## [57] 22.84551 16.27916 28.03817 24.16301 31.17074 20.02190 26.56155 20.62508 ## [65] 18.92214 20.38837 27.83197 18.62320 19.12094 29.29845 22.72270 28.68786 ## [73] 22.64878 30.43969 19.48136 24.94834 28.74896 24.99252 21.52521 22.19714 ## [81] 22.72903 23.69804 30.32393 29.37573 29.65554 22.18997 24.85922 21.19787 ## [89] 27.94906 28.93757 27.42930 23.87127 33.93523 21.89126 26.24338 29.48277 ## [97] 26.96407 19.72505 28.14750 24.06552 Vamos observar as métricas e gráficos, conforme já fizemos nos capítulos anteriores, mas relativos ao objeto criado. mean(tamanho.camarao) ## [1] 24.86218 min(tamanho.camarao) ## [1] 16.27916 max(tamanho.camarao) ## [1] 33.93523 length(tamanho.camarao) ## [1] 100 hist(tamanho.camarao) Figura 4.1: Histograma dos valores do objeto relativo ao tamanho dos camarões Como podemos ver os valores mínimo e máximo são similares ao que definimos e o número de elementos é o mesmo. Graficamente podemos ver que a distribuição é normal, pois como vimos utilizamos uma função que cria uma distribuição normal com limites definidos pela função runif(). Além disso vemos que há uma maior frequência dos valores em torno de 25 (Figura 4.1). Vamos verificar se esse valor que geramos (obtivemos de tamanho do camarão) são similares ao observado em outra localidade, por meio do teste-t para uma amostra. Para isso imaginemos que o valor médio esperado para o tamanho desta espécie de camarão em outra localidade é de 25,80mm. Para isso iremos criar o objeto esperado que contem esse valor. esperado = 25.80 Agora vamos, por meio da função t.test() realizar o teste-t para verificar se os nossos dados tem média similar ao esperado. t.test(x = tamanho.camarao, mu = esperado) ## ## One Sample t-test ## ## data: tamanho.camarao ## t = -2.2432, df = 99, p-value = 0.02711 ## alternative hypothesis: true mean is not equal to 25.8 ## 95 percent confidence interval: ## 24.03263 25.69173 ## sample estimates: ## mean of x ## 24.86218 O primeiro argumento da função t.test(): “x” refere-se ao nosso objeto que contem os dados referentes ao tamanho que coletamos (tamanho.camarao) e o segundo argumento “mu” refere-se ao objeto que contem a média do tamanho obtido em outro estuário (esperado). Conforme podemos visualizar no resultado temos 9 linhas. A primeira linha nos diz qual teste está sendo conduzido, neste caso é (One Sample t-test ou teste-t para uma amostra), a segunda linha nos retorna o conjunto de dados que utilizamos, a terceira linha nos retorna o valor do teste-t (t = -2,2432) o grau de liberdade (df = 99) e o valor de probabilidade associado ao teste (p-value = 0,02711), a quarta linha nos retorna qual é nossa hipótese alternativa, caso a aceitemos (a qual nos diz que: a média dos nossos dados não é igual à 25,8), a quinta e sexta linhas nos fornece o intervalo de confiança de 95% dos nossos dados (24,03263 e 25,69173) e da sétima a nona linha refere-se a informação relativa a média dos nossos dados (24,86218). Em resumo, podemos inferir que a média dos nossos dados é diferente do valor esperado pois o p-value foi menor que 0,05. Detalhadamente podemos dizer que: a média do tamanho dos camarões que coletamos (24,86 mm) é estatisticamente menor do que a média presente no outro estuário (25,80 mm) a um nível de confiança de 95%, portanto rejeitamos H0. Ok, verificamos e entendemos como conduzir a análise. Mas há um conceito estatístico importante na análise do teste-t, a caudalidade. No exemplo anterior nós trabalhamos com as hipóteses de que a média de um conjunto de dados é igual (H0) ou diferente (HA) da média esperada. O que implica em dizer que a média que observamos pode ser maior ou menor do que o esperado. Contudo em algumas instâncias podemos querer verificar se a média do nosso conjunto de dados é maior ou igual ou menor ou igual a média esperada e não diferente. Desta diferença na construção da hipótese que emerge o conceito da caudalidade. No exemplo anterior foi testado a hipótese bicaudal que é tido como padrão (default) na função do R que executamos. A diferença do teste-t bicaudal para o unicaudal depende da pergunta e hipótese levantada previamente. Então antes de realizar este teste mantenha-se atento ao que se deseja testar. Vejamos o seguinte exemplo: Em um costão rochoso foi observado ao longo do “dia 1” 100 estrelas do mar em diferentes alturas, em relação a baixamar. Essas alturas foram mensuradas (valores serão construídos abaixo no objeto denominado estrela). Sabe-se que no dia anterior “dia 0” a altura média das estrelas no mesmo costão foi de 0,90m. Sabe-se também que o “dia 0” foi mais frio e que dias mais frios implicam em maiores alturas. Neste exemplo estamos interessado em saber se a altura das estrelas do mar no “dia 1” é menor que a do “dia 0”, visto que o “dia 1” é mais quente. A partir dessas informações podemos construir as seguintes hipóteses: H0: A média da altura das estrelas do mar no dia 1 é maior ou igual ao dia 0; HA: A média da altura das estrelas do mar no dia 1 é menor que a do dia 0; Matematicamente podemos escrever as hipóteses da seguinte forma: H0: \\(\\mu_{dia.1} \\geq \\mu_{dia.0}\\) HA: \\(\\mu_{dia.1} &lt; \\mu_{dia.0}\\) De maneira similar ao exemplo anterior vamos gerar nosso conjunto de dados que contêm os valores de altura das estrelas do mar no costão rochoso, a diferença é que iremos inserir o argumento “sd” na função rnorm() que corresponde ao desvio padrão (de 0,02) dos dados normais que estamos gerando e guardar esse resultado no objeto estrelas. set.seed(1234) estrelas &lt;- rnorm(n = 100, sd = 0.02) + runif(n = 100, min = 0.4, max = 1.3) Conforme também fizemos anteriormente calcularemos algumas métricas para entender os dados e um histograma básico (Figura 4.2) para visualizar a forma dos dados que representam a altura que as estrelas se encontram no ambiente. mean(estrelas) ## [1] 0.8479302 min(estrelas) ## [1] 0.387487 max(estrelas) ## [1] 1.290766 length(estrelas) ## [1] 100 hist(estrelas) Figura 4.2: Histograma dos valores relativos a altura das estrelas do mar no costão rochoso Agora vamos criar o objeto dia.0 que contêm o valor referente a altura das estrelas no dia 0. dia.0 = 0.90 Agora o teste-t onde avaliaremos se a média da altura das estrelas do mar que obtivemos para o “dia 1” são maiores ou iguais ao “dia 0”, utilizando o argumento “alternative” e definido-o como less. Este argumento pode ser definido de 3 formas: two-sided que é o padrão (default), greater ou less. O argumento “alternative” seguirá o sinal da hipótese alternativa (HA) que foi construída para o teste. t.test(x = estrelas, mu = dia.0, alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: estrelas ## t = -2.2073, df = 99, p-value = 0.0148 ## alternative hypothesis: true mean is less than 0.9 ## 95 percent confidence interval: ## -Inf 0.8870986 ## sample estimates: ## mean of x ## 0.8479302 Com isso podemos avaliar o resultado que é similar ao que vimos anteriormente com poucas mudanças. A primeira linha (One Sample t-test) informa sobre o teste realizado. A segunda linha indica o nome do conjunto de dados que inserimos. A terceira linha nos dá o valor do teste-t (t = -2,2073), do grau de liberdade (df = 99) e da probabilidade associada ao teste (p-value = 0.0148). A quarta linha indica a hipótese alternativa, caso seja aceita (o que é o caso), que é verdade que a média da altura das estrelas do mar do “dia 1” é menor que 0,90m. A quinta e sexta linha indicam o intervalo de confiança de 95% dos nossos dados (-Inf e 0,8870986). A sétima, oitava e nona linha referem-se a média do “dia 1”. Como podemos notar pelo p-value rejeitamos nossa hipótese nula (H0). Portanto a média na altura das estrelas do mar do “dia 1” (aproximadamente 0,85m) é menor que a do dia “0” (0,90m). Outro conceito importante de qualquer teste inferencial é o nível de confiança16. Até o presente momento consideramos o nível de confiança de 95%. Se quisermos altera-lo no teste-t devemos adicionar o argumento “conf.level” em proporção (valores entre 0 e 1). A sua alteração implica na alteração do nível de significância e consequentemente na zona de rejeição da hipótese nula. Se aumentarmos o seu valor fica mais difícil rejeitarmos a hipótese nula e se diminuirmos o seu valor fica mais fácil rejeitar a hipótese nula. Vejamos outro exemplo. Um pesquisador avaliou o tamanho de cracas incrustadas no casco de uma embarcação. Objetivando saber se o tamanho médio de cracas difere do teórico esperado (13,5 mm) um teste-t bicaudal foi aplicado a um nível de confiança de 95% e 99%. Vamos descrever as hipóteses e realizar a análise para ambos os níveis de confiança. Neste caso temos as seguintes hipóteses: H0: O tamanho médio observado é similar ao teórico HA: O tamanho médio observado difere do teórico Conforme já realizado anteriormente vamos gerar os dados iguais aos gerados aqui por meio das funções set.seed(), rnorm() e runif(), guardando o seu resultado em um objeto chamado cracase explora-los com algumas métricas estatśticas e gráficas de maneira similar ao que fizemos no exemplo anterior. set.seed(1234) cracas &lt;- rnorm(n = 100, sd = 1.9) + runif(n = 100, min = 5, max = 25) mean(cracas) ## [1] 14.72583 min(cracas) ## [1] 3.596275 max(cracas) ## [1] 27.27792 length(cracas) ## [1] 100 hist(cracas) Figura 4.3: Histograma dos valores relativos ao tambanho das cracas incrustantes Agora que visualizamos as métricas e graficamos os dados (Figura 4.3), sigamos construindo o objeto que guarda o valor teórico (13,5mm) e com a condução da análise. teorico &lt;- 13.5 t.test(x = cracas, mu = teorico, conf.level = 0.95) ## ## One Sample t-test ## ## data: cracas ## t = 2.3113, df = 99, p-value = 0.02289 ## alternative hypothesis: true mean is not equal to 13.5 ## 95 percent confidence interval: ## 13.67349 15.77817 ## sample estimates: ## mean of x ## 14.72583 t.test(x = cracas, mu = teorico, conf.level = 0.99) ## ## One Sample t-test ## ## data: cracas ## t = 2.3113, df = 99, p-value = 0.02289 ## alternative hypothesis: true mean is not equal to 13.5 ## 99 percent confidence interval: ## 13.33290 16.11876 ## sample estimates: ## mean of x ## 14.72583 Como podem ver ambos os resultados (com diferentes níveis de confiança) retornam o mesmo valor do teste-t e do p-value. E neste ponto precisamos ir com calma para evitar erro de interpretação do resultado e entender estatisticamente o que está acontencendo. Quando representamos o nível de confiança (representado pelo argumento “conf.level”) por um valor probabilístico de 0,95 ou 0,99 estamos dizendo que o nível de significância é 0,05 e 0,01, respectivamente. Quando olhamos para o resultado do p-value, temos que levar em consideração o nível de significância. Vejamos o nosso resultado. No primeiro caso (“conf.level = 0.95”) temos p-value = 0,02289, como este valor é menor que 0,05 (nosso nível de significância), isso quer dizer que a média teórica está fora do intervalo de confiança dos dados, portanto rejeitamos a hipótese nula. No segundo caso (“conf.level = 0.99”) temos o mesmo p-value (0,02289), contudo este valor é maior que nosso nível de significância (0,01), isso quer dizer que nossa média teórica está dentro do intervalo de confiança, portanto aceitamos a hipótese nula de que a média observada é similar a média teórica. OBS: Em um primeiro momento pode parecer confuso, portanto releia o exemplo acima, assim como outras literaturas. O nível de significância a aplicar nos seus dados depende das informações que possui sobre o organismo ou o ambiente que está estudando. Apesar da regra-de-bolso dizer 0,05 e por padrão o R definir esse nível de significância é necessário entender o que ele representa para seus dados e qual a implicação para sua hipótese e as medidas que serão tomadas. Uma dica importante é: reporte sempre o intervalo de confiança, indique o nível de significância que foi aplicado e no seu texto deixe claro o porquê de sua escolha, principalmente se for diferente do que é definido como padrão. BÔNUS: Embora não seja comum, podemos plotar um gráfico que represente o nosso resultado estatístico como uma curva de densidade no qual é representado o valor do teste-t, o grau de liberdade e o valor de probabilidade associado. Para isso vamos usar um pacote o qual precisa ser instalado chamado webr e precisamos carrega-lo usando a função library(). Após isso é só inserir a função que desenvolve o teste-t dentro da função plot(). Veja o resultado para ambos os níveis de confiança estabelecidos previamente. OBS: Uma vez instalado o pacote não precisa instala-lo novamente apenas carrega-lo se estiver iniciando o R pela primeira vez. install.packages(&quot;webr&quot;) library(webr) Após instalar e carregar o pacote devemos inserir o teste-t dentro da função plot(). Veja os dois exemplos abaixo. plot(t.test(x = cracas, mu = teorico, conf.level = 0.95)) Figura 4.4: Curva de densidade representando o valor do teste-t bicaudal para o tamanho das cracas em relação a média teórica a um intervalo de confiança de 95%. O ponto azul indica o valor do teste. plot(t.test(x = cracas, mu = teorico, conf.level = 0.99)) Figura 4.5: Curva de densidade representando o valor do teste-t bicaudal para o tamanho das cracas em relação a média teórica a um intervalo de confiança de 99%. O ponto azul indica o valor do teste. Verifique que esses plots nos fornecem as curvas do teste-t os intervalos de confiança de 95% (Figura 4.4) e 99% (Figura 4.5), demarca os limites inferior e superior de vermelho e marca como ponto azul na curva de densidade o valor do teste-t associado. Como esse valor está dentro da região demarcada no intervalo de confiança de 95%, neste caso rejeitamos a hipótese nula e como no intervalo de confiança de 99% o ponto azul está fora da região demarcada aceitamos a hipótese nula. 4.2 Teste-t para duas amostras Conduzimos esse teste quando objetivamos comparar se a média de dois grupos são similares (Tabela 4.2). Tabela 4.2: Principais características do teste-t para duas amostras Atributos Características Tipo de variável Quantitativa e categórica Quantidade de variáveis 2 (1 de cada tipo obrigatoriamente) Hipótese nula A diferença na média da variável quantitativa dos grupos é igual a 0. Fórmula \\[t=\\frac{\\overline{X}_1-\\overline{X}_2}{s_{\\overline{X}_1-\\overline{X}_2}}\\], onde, \\(\\overline{X}_1\\): média do grupo 1, \\(\\overline{X}_2\\): média do grupo 2, s: erro padrão da diferença entre os dois grupos. Observação Não há a necessidade de post-hoc nem expressa-la graficamente. Partindo desse objetivo as hipóteses nula e alternativa desse teste podem ser escritas da seguinte forma: H0: A diferença na média da variável quantitativa dos grupos é igual a 0; HA: A diferença na média da variável quantitativa dos grupos é diferente de 0; Outra forma de apresentarmos as hipóteses relativa a este teste é: H0: A média da variável quantitativa é igual entre grupos; HA: A média da variável quantitativa é diferente entre grupos; Outra forma de escrevermos a hipótese é em relação a caudalidade do teste e, se unicaudal, pode ser escrita da seguinte forma: H0: A média da variável quantitativa é maior ou igual (ou menor ou igual) entre grupos; HA: A média da variável quantitativa é menor (ou maior) entre grupos; A partir desse teste, alguns pressupostos estatísticos precisam ser avaliados e portanto algumas análises precisam ser realizadas antes da interpretação do resultado do teste-t. Pressupostos como normalidade (ambos os grupos devem provir de uma população com distribuição normal) e homocedasticidade (A variância entre os dois grupos devem ser iguais). Demonstraremos a frente como realizar alguns desses testes. Mas, primeiro vamos gerar os dados a serem trabalhados para esta etapa. Execute o código abaixo e você irá visualizar no environment do seu RStudio um objeto chamado gastropode que corresponde a um data frame com o conjunto de dados que iremos trabalhar. OBS: Neste momento não entraremos em detalhes sobre os comandos aplicados para construção dos dados. grupos &lt;- c(rep(x = &quot;Alimento A&quot;, 50), rep(x = &quot;Alimento B&quot;, 50)) set.seed(123) alimento.A &lt;- rnorm(n = 50, mean = 0.7, sd = 0.2) set.seed(4321) alimento.B &lt;- rnorm(n = 50, mean = 0.2, sd = 0.2) Alimento &lt;- c(alimento.A, alimento.B) gastropode &lt;- as.data.frame(cbind(grupos, round(Alimento, 3))) colnames(gastropode) &lt;- c(&quot;Alimento&quot;, &quot;Peso&quot;) rm(list = &quot;grupos&quot;, &quot;alimento.A&quot;, &quot;alimento.B&quot;, &quot;Alimento&quot;) Vamos, agora, verificar os dados por meio da função head() e a estrutura dos dados por meio da função str() e alterar a estrutura dos dados se necessário. head(gastropode) ## Alimento Peso ## 1 Alimento A 0.588 ## 2 Alimento A 0.654 ## 3 Alimento A 1.012 ## 4 Alimento A 0.714 ## 5 Alimento A 0.726 ## 6 Alimento A 1.043 str(gastropode) ## &#39;data.frame&#39;: 100 obs. of 2 variables: ## $ Alimento: chr &quot;Alimento A&quot; &quot;Alimento A&quot; &quot;Alimento A&quot; &quot;Alimento A&quot; ... ## $ Peso : chr &quot;0.588&quot; &quot;0.654&quot; &quot;1.012&quot; &quot;0.714&quot; ... gastropode$Alimento &lt;- as.factor(gastropode$Alimento) gastropode$Peso &lt;- as.numeric(gastropode$Peso) str(gastropode) ## &#39;data.frame&#39;: 100 obs. of 2 variables: ## $ Alimento: Factor w/ 2 levels &quot;Alimento A&quot;,&quot;Alimento B&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Peso : num 0.588 0.654 1.012 0.714 0.726 ... Agora que importamos e organizamos nossa planilha vamos analisar nosso exemplo. 100 indivíduos de uma espécie de gastropode foi coletada e mantida em cultivo para avaliação da dieta. 50 indivíduos foram mantidos com uma dieta rica no Alimento A e 50 indivíduos com uma dieta rica no Alimento B. Todos os indivíduos foram pesados antes e depois do experimento. A planilha a seguir informa a alteração de peso dos organismos, em gramas, após a dieta oferecida. Antes de conduzirmos o teste-t vamos praticar fazendo a avaliação gráfica e númerica dos dados. Os comandos abaixo realizam: o histograma dos dados para Alimento A (Figura 4.6), o histograma dos dados para Alimento B (Figura 4.7) e o boxplot para ambos os dados (Figura 4.8), respectivamente. Os 2 primeiros gráficos consistem em histogramas em relação aos dados de peso por Alimento. Na primeira linha indicaremos a variável peso dentro da planilha gastropode por meio do operador matemático “$” (cifrão) e selecionamos os dados correspondentes ao Alimento utilizando colchetes [] dentro do qual selecionamos a variável Alimento dentro da planilha gastropode e por meio do sinal de igual duplicado (==) indicamos entre aspas (&quot;“) o grupo (ou categoria) que desejamos. Os demais argumentos já são bem conhecidos e iguais para ambos os histogramas, diferindo apenas o título do gráfico que é definido pelo argumento”main&quot;. hist(gastropode$Peso [gastropode$Alimento == &quot;Alimento A&quot;], xlab = &quot;Alteração do peso&quot;, ylab = &quot;Frequência&quot;, main = &quot;Alimento A&quot;, ylim = c(0, 20), xlim = c(-0.5, 1.5)) Figura 4.6: Histograma com valores da mudança do peso dos gastrópodes após a dieta com o Alimento A hist(gastropode$Peso [gastropode$Alimento == &quot;Alimento B&quot;], xlab = &quot;Alteração do peso&quot;, ylab = &quot;Frequência&quot;, main = &quot;Alimento B&quot;, ylim = c(0, 20), xlim = c(-0.5, 1.5)) Figura 4.7: Histograma com valores da mudança do peso dos gastrópodes após a dieta com o Alimento B O Boxplot resume os dados por Alimento e nos indica outras métricas (quartis), como vimos anteriormente no tópico sobre gráficos. boxplot(Peso ~ Alimento, data = gastropode) Figura 4.8: Boxplot com os valores da alteração do peso dos gastrópodes por tipo de alimento Mas se desejarmos observar os valores numéricos que resumem os dados, podemos seguir o que aprendemos anteriormente. summary(gastropode) ## Alimento Peso ## Alimento A:50 Min. :-0.1180 ## Alimento B:50 1st Qu.: 0.2172 ## Median : 0.4475 ## Mean : 0.4652 ## 3rd Qu.: 0.6843 ## Max. : 1.1340 mean(gastropode$Peso) ## [1] 0.46519 mean(gastropode$Peso [gastropode$Alimento == &quot;Alimento A&quot;]) ## [1] 0.7069 mean(gastropode$Peso [gastropode$Alimento == &quot;Alimento B&quot;]) ## [1] 0.22348 Utilizando o pacote Rmisc temos uma forma mais simples de escrita e eficiente para observar esses valores e algumas outras métricas. OBS: Não se esqueça de carregar o pacote por meio da função library(). library(Rmisc) summarySE(data = gastropode, measurevar = &quot;Peso&quot;, groupvars = &quot;Alimento&quot;) ## Alimento N Peso sd se ci ## 1 Alimento A 50 0.70690 0.1852070 0.02619223 0.05263525 ## 2 Alimento B 50 0.22348 0.1573551 0.02225337 0.04471982 Até o momento observamos como estão os nossos dados e podemos ver que a administração do “Alimento A” resultou em um maior ganho de peso pelos gastropodes do que a “Alimento B”. Mas será que o que observamos grafica e numericamente se reflete estatisticamente? Vamos a nossa avaliação dos pressupostos do teste-t para duas amostras e se cumpridos vamos para o teste-t. Uma das formas mais convencionais de avaliar a normalidade é pelo teste de Shapiro-Wilks aplicando a função shapiro.test() e a homocedasticidade pelo teste de Bartlett bartlett.test(). Para o teste de normalidade de Shapiro-Wilks devemos aplica-lo à variável númerica desejada (ex: Peso) e realiza-lo para cada categória (ex: “Alimento A” e “Alimento B”) da variável categorica escolhida (ex: Alimento) separadamente. Para o teste de homocedasticidade de Bartlett devemos avaliar a variável mensurável (ex: Peso) em função da variável categorica (ex: Alimento). Veja os comandos e resultado abaixo. shapiro.test(gastropode$Peso [gastropode$Alimento == &quot;Alimento A&quot;]) ## ## Shapiro-Wilk normality test ## ## data: gastropode$Peso[gastropode$Alimento == &quot;Alimento A&quot;] ## W = 0.98923, p-value = 0.9266 shapiro.test(gastropode$Peso [gastropode$Alimento == &quot;Alimento B&quot;]) ## ## Shapiro-Wilk normality test ## ## data: gastropode$Peso[gastropode$Alimento == &quot;Alimento B&quot;] ## W = 0.98059, p-value = 0.5769 bartlett.test(Peso ~ Alimento, data = gastropode) ## ## Bartlett test of homogeneity of variances ## ## data: Peso by Alimento ## Bartlett&#39;s K-squared = 1.2826, df = 1, p-value = 0.2574 Como podemos observar, ambos os grupos apresentam dados normais e homocedásticos, para um nível de confiança de 95%, já que o p-value foi superior a 0,05. Dessa forma vamos dar continuidade a nossa análise e verificar se as médias dos grupos são diferentes aplicando a já conhecida função t.test(). t.test(Peso ~ Alimento, data = gastropode, var.equal = TRUE, conf.level = 0.95) ## ## Two Sample t-test ## ## data: Peso by Alimento ## t = 14.065, df = 98, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.4152153 0.5516247 ## sample estimates: ## mean in group Alimento A mean in group Alimento B ## 0.70690 0.22348 Repare que a forma da escrita se alterou um pouco. Mas como podem ver, nada complicado. Agora escrevemos a variável mensurável (Peso) em função da (~) variável categórica (Alimento). Guarde bem essa forma de escrita (similar ao que realizamos no boxplot e barplot) pois ela será utilizada para praticamente todos os testes a partir daqui e para inúmeras outras funções. Adcionamos o argumento “data” que indica a planilha de onde estamos utilizando as variáveis, o argumento “var.equal” o qual indica que a variância entre os grupos é igual17 e o argumento “conf.level” o qual define o nível de confiança com qual estamos trabalhando. De acordo com nosso resultado podemos ver que o valor do teste-t é 14,065, o grau de liberdade de 98 (o qual consiste no total de observações subtraído de um por grupo), o valor de probabilidade associado ao teste (p-value = \\(2,2\\times10^{-16}\\)), o intervalo de confiança de 95% (0,415 e 0,551) que refere-se a diferença da média entre os grupos (a diferença da média dos grupos é: 0,70690 - 0,22348 = 0,48342), ou seja, o intervalo de confiança é em função dessa diferença e as últimas linhas do resultado representam as médias de alteração do peso para cada Alimento (Alimento A = 0,70690 e Alimento B = 0,22348). De acordo com esse resultado refutamos a hipótese nula de que as médias são similares. Portanto podemos dizer que dependendo do Alimento (A ou B) utilizada na dieta podemos ter diferentes alterações no peso dos gastropodes. Da mesma forma que avaliamos para o teste-t de uma amostra, podemos plotar o resultado como um gráfico da função de densidade do teste-t. Só devemos lembrar de carregar o pacote webr. library(webr) plot(t.test(Peso ~ Alimento, data = gastropode, var.equal = TRUE, conf.level = 0.95)) Figura 4.9: Curva de densidade representando o valor do teste-t bicaudal para a alteração do peso de gastrópodes em relação ao alimento a um intervalo de confiança de 95%. O ponto azul indica o valor do teste. Neste gráfico podemos ver que o resultado do teste-t (Figura 4.9), indicado pelo ponto azul, está muito além do nível da zona de rejeição, indicando que os dois grupos apresentam médias bem diferentes, ou seja, a diferença entre as duas médias é altamente significativa. Vamos exercitar nosso conhecimento em R e teste-t com um outro exemplo (Gere os dados abaixo). OBS: Neste momento não entraremos em detalhes sobre os comandos aplicados para construção dos dados. grupos &lt;- c(rep(x = &quot;Ano 0&quot;, 36), rep(x = &quot;Ano 20&quot;, 36)) set.seed(245) ano.0 &lt;- rnorm(n = 36, mean = 15.7, sd = 0.2) + runif(n = 36) set.seed(356) ano.20 &lt;- rnorm(n = 36, mean = 16.2, sd = 0.5) + runif(n = 36) ano &lt;- c(ano.0, ano.20) lagoa &lt;- as.data.frame(cbind(grupos, round(ano, 3))) colnames(lagoa) &lt;- c(&quot;Ano&quot;, &quot;Temperatura&quot;) rm(list = &quot;grupos&quot;, &quot;ano.0&quot;, &quot;ano.20&quot;, &quot;ano&quot;) Imagine que durante um ano você mensurou a temperatura de uma lagoa três vezes por mês durante todos os meses ao longo de um ano. 20 anos depois você retornou a lagoa e mensurou novamente a temperatura três vezes por mês durante um ano. Considerando um nível de confiança de 99% a temperatura é igual ou diferente entre os anos? A primeira coisa que devemos fazer é escrever nossa hipótese. Vamos a ela. H0: A média da temperatura é igual entre os anos (\\(\\mu_{ano0} = \\mu_{ano 20}\\)); HA: A média da temperatura é diferente entre os anos (\\(\\mu_{ano0} \\neq \\mu_{ano 20}\\)); Com a hipótese construída vamos verificar a estrutura dos dados (modificar se necessário) e sumarizar nossos dados grafica e estatisticamente. Conforme realizado no exemplo anterior. Os comandos abaixo realizam a análise gráfica: o histograma dos valores de temperatura no “ano 0” (Figura 4.10), o histograma dos valores de temperatura no “ano 20” (Figura 4.11) e o boxplot para ambos os dados (Figura 4.12), respectivamente. Quanto ao seu resultado podemos notar uma maior temperatura média anual da lagoa 20 anos depois da primeira amostragem. head(lagoa) ## Ano Temperatura ## 1 Ano 0 16.345 ## 2 Ano 0 16.383 ## 3 Ano 0 15.727 ## 4 Ano 0 16.058 ## 5 Ano 0 15.842 ## 6 Ano 0 16.158 str(lagoa) ## &#39;data.frame&#39;: 72 obs. of 2 variables: ## $ Ano : chr &quot;Ano 0&quot; &quot;Ano 0&quot; &quot;Ano 0&quot; &quot;Ano 0&quot; ... ## $ Temperatura: chr &quot;16.345&quot; &quot;16.383&quot; &quot;15.727&quot; &quot;16.058&quot; ... lagoa$Ano &lt;- as.factor(lagoa$Ano) lagoa$Temperatura &lt;- as.numeric(lagoa$Temperatura) str(lagoa) ## &#39;data.frame&#39;: 72 obs. of 2 variables: ## $ Ano : Factor w/ 2 levels &quot;Ano 0&quot;,&quot;Ano 20&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Temperatura: num 16.3 16.4 15.7 16.1 15.8 ... hist(lagoa$Temperatura [lagoa$Ano == &quot;Ano 0&quot;], xlab = &quot;Temperatura (°C)&quot;, ylab = &quot;Frequência&quot;, main = &quot;Ano 0&quot;, ylim = c(0, 15), xlim = c(15, 19)) Figura 4.10: Histograma com valores de temperatura da lagoa no Ano 0. hist(lagoa$Temperatura [lagoa$Ano == &quot;Ano 20&quot;], xlab = &quot;Temperatura (°C)&quot;, ylab = &quot;Frequência&quot;, main = &quot;Ano 20&quot;, ylim = c(0, 15), xlim = c(15, 19)) Figura 4.11: Histograma com valores de temperatura da lagoa no Ano 20. boxplot(Temperatura ~ Ano, data = lagoa) Figura 4.12: Boxplot com os valores de temperatura da lagoa por ano. Abaixo temos o comando para realizar a sumarização numérica dos dados. summary(lagoa) ## Ano Temperatura ## Ano 0 :36 Min. :15.58 ## Ano 20:36 1st Qu.:16.08 ## Median :16.41 ## Mean :16.52 ## 3rd Qu.:16.86 ## Max. :18.74 library(Rmisc) summarySE(data = lagoa, measurevar = &quot;Temperatura&quot;, groupvars = &quot;Ano&quot;) ## Ano N Temperatura sd se ci ## 1 Ano 0 36 16.14661 0.3301591 0.05502652 0.1117098 ## 2 Ano 20 36 16.89750 0.6409853 0.10683089 0.2168782 Aplicando a função summarySE() do pacote Rmisc sumarizamos nossos dados como mostrado acima e em relação aos valores de média e desvios podemos observar que a média é bem próxima, mas será que elas são estatisticamente iguais? Para isso vamos realizar o teste-t. Antes do teste-t vamos calcular os pressupostos requeridos, normalidade e homocedasticidade. Conforme realizado no exemplo anterior. shapiro.test(lagoa$Temperatura [lagoa$Ano == &quot;Ano 0&quot;]) ## ## Shapiro-Wilk normality test ## ## data: lagoa$Temperatura[lagoa$Ano == &quot;Ano 0&quot;] ## W = 0.97181, p-value = 0.4771 shapiro.test(lagoa$Temperatura [lagoa$Ano == &quot;Ano 20&quot;]) ## ## Shapiro-Wilk normality test ## ## data: lagoa$Temperatura[lagoa$Ano == &quot;Ano 20&quot;] ## W = 0.9571, p-value = 0.1747 bartlett.test(Temperatura ~ Ano, data = lagoa) ## ## Bartlett test of homogeneity of variances ## ## data: Temperatura by Ano ## Bartlett&#39;s K-squared = 14.189, df = 1, p-value = 0.0001653 Como podemos ver os dados são normais, porém não são homocedasticos (a variância não é igual entre os grupos). Neste caso podemos fazer um teste-t de Welch (este teste aplica uma correção quando as variâncias não são iguais). Contudo, o teste de Welch ele é comumente usado quando o N amostral é considerado baixo (menor que 10 para um dos dois grupos). Vamos Analisar o teste-t considerando a variância igual e desigual para ver se há diferença significativa no resultado do teste ou não. t.test(Temperatura ~ Ano, data = lagoa, var.equal = TRUE, conf.level = 0.99) ## ## Two Sample t-test ## ## data: Temperatura by Ano ## t = -6.2486, df = 70, p-value = 2.837e-08 ## alternative hypothesis: true difference in means is not equal to 0 ## 99 percent confidence interval: ## -1.069087 -0.432691 ## sample estimates: ## mean in group Ano 0 mean in group Ano 20 ## 16.14661 16.89750 t.test(Temperatura ~ Ano, data = lagoa, var.equal = FALSE, conf.level = 0.99) ## ## Welch Two Sample t-test ## ## data: Temperatura by Ano ## t = -6.2486, df = 52.35, p-value = 7.594e-08 ## alternative hypothesis: true difference in means is not equal to 0 ## 99 percent confidence interval: ## -1.0721092 -0.4296686 ## sample estimates: ## mean in group Ano 0 mean in group Ano 20 ## 16.14661 16.89750 Como podem notar a forma de escrever o teste é similar ao exemplo anterior as alterações consistem nas variáveis e no conjunto de dados utilizado e como foi pedido no teste a alteração do nível de confiança para 99% (conf.level = 0,99) e os dois teste-t (com variância igual e com variância desigual - Welch). Olhando para os dois resultados ambos os testes demonstraram diferenças significativas entre os anos, pois o p-value foi menor que 0,01 (lembrar que como o nível de confiança foi alterado para 0,99 a significância só ocorrerá se o p-value for menor que 0,01, como é o caso). Porém podemos ver que não há muita diferença em relação aos valores de ambos os testes-t, pois como ressaltamos a aplicação do teste de Welch apresenta maior peso quando as variâncias são desiguais e o número amostral de um dos grupos é muito pequeno (&lt;10). Assim como fizemos anteriormente vamos olhar o resultado em relação a distribuição da função de densidade do teste-t para um nível de confiança de 99% (Figura 4.13). Só devemos lembrar de carregar o pacote webr, se ainda não foi carregado. library(webr) plot(t.test(Temperatura ~ Ano, data = lagoa, var.equal = TRUE, conf.level = 0.99)) Figura 4.13: Curva de densidade representando o valor do teste-t bicaudal para comparação da temperatura em um lago entre dois anos a um intervalo de confiança de 99%. O ponto azul indica o valor do teste. De acordo com esse exemplo podemos afirmar que os anos diferem entre si e que em 20 anos a lagoa amostrada apresentou um aumento da temperatura. 4.3 Teste-t pareado Aplicamos este teste quando as duas amostras de uma variável categórica são independentes e desejamos verificar se os pares de observações entre os grupos diferem entre si (Tabela 4.3). Cada observação de ambas as amostras devem estar associadas para podermos dizer que ocorrem em pares. Tabela 4.3: Principais características do teste-t pareado Atributos Características Tipo de variável Quantitativa e categórica Quantidade de variáveis 3 (1 quantitativa e 2 categóricas) Hipótese nula A diferença na média da variável quantitativa entre os pares de grupos é igual a 0. Fórmula \\[t=\\frac{\\overline{d}}{s_{\\overline{d}}}\\], onde, \\(\\overline{d}\\): média da diferença entre os pares de observações entre os grupos, \\(s_{\\overline{d}}\\): erro padrão da diferença entre os pares de observações entre os grupos. Observação Não há a necessidade de post-hoc nem expressa-la graficamente. Vejamos um exemplo de como conduzir essa análise. Imagine que dois pesquisadores embarcaram com objetivo de fazer contagem de aves em alto mar. Após 20 dias de observações, independentes entre os observadores, obtivemos os dados abaixo. Começaremos gerando os dados e o modificando conforme o necessário. OBS: Neste momento não entraremos em detalhes sobre os comandos aplicados para construção dos dados. set.seed(2328) aves.1 &lt;- rnorm(n = 20, mean = 20, sd = 1) + runif(n = 20) set.seed(3230) aves.2 &lt;- rnorm(n = 20, mean = 21, sd = 2) + runif(n = 20) dia &lt;- c(&quot;Dia 1&quot;, &quot;Dia 2&quot;, &quot;Dia 3&quot;, &quot;Dia 4&quot;, &quot;Dia 5&quot;, &quot;Dia 6&quot;, &quot;Dia 7&quot;, &quot;Dia 8&quot;, &quot;Dia 9&quot;, &quot;Dia 10&quot;, &quot;Dia 11&quot;, &quot;Dia 12&quot;, &quot;Dia 13&quot;, &quot;Dia 14&quot;, &quot;Dia 15&quot;, &quot;Dia 16&quot;, &quot;Dia 17&quot;, &quot;Dia 18&quot;, &quot;Dia 19&quot;, &quot;Dia 20&quot;) aves &lt;- as.data.frame(cbind(dia, round(aves.1, 0), round(aves.2, 0))) colnames(aves) &lt;- c(&quot;Dia&quot;, &quot;Observador.1&quot;, &quot;Observador.2&quot;) rm(list = &quot;aves.1&quot;, &quot;aves.2&quot;, &quot;dia&quot;) Nestes casos temos as seguintes hipoteses: H0: A diferença na média do número de observações de aves entre os Observadores é igual a 0. HA: A diferença na média do número de observações de aves entre os Observadores difere de 0. Vamos verificar a estrutura dos dados e modificar conforme a necessidade. head(aves) ## Dia Observador.1 Observador.2 ## 1 Dia 1 19 20 ## 2 Dia 2 20 22 ## 3 Dia 3 18 23 ## 4 Dia 4 22 18 ## 5 Dia 5 22 22 ## 6 Dia 6 22 23 str(aves) ## &#39;data.frame&#39;: 20 obs. of 3 variables: ## $ Dia : chr &quot;Dia 1&quot; &quot;Dia 2&quot; &quot;Dia 3&quot; &quot;Dia 4&quot; ... ## $ Observador.1: chr &quot;19&quot; &quot;20&quot; &quot;18&quot; &quot;22&quot; ... ## $ Observador.2: chr &quot;20&quot; &quot;22&quot; &quot;23&quot; &quot;18&quot; ... aves$Dia &lt;- as.factor(aves$Dia) aves$Observador.1 &lt;- as.numeric(aves$Observador.1) aves$Observador.2 &lt;- as.numeric(aves$Observador.2) str(aves) ## &#39;data.frame&#39;: 20 obs. of 3 variables: ## $ Dia : Factor w/ 20 levels &quot;Dia 1&quot;,&quot;Dia 10&quot;,..: 1 12 14 15 16 17 18 19 20 2 ... ## $ Observador.1: num 19 20 18 22 22 22 21 20 22 21 ... ## $ Observador.2: num 20 22 23 18 22 23 20 24 22 24 ... Neste exemplo podemos perceber que temos as variáveis categóricas “Dia” e observadores os quais estão em colunas como: “Observador 1” e “Observador 2” e a variável quantitativa “aves” que consiste no número de aves avistadas. Vamos sumarizar os dados grafica e estatisticamente. Primeiro vejamos nossos histogramas relativos a observações em relação aos observadores (Figura 4.14 e 4.15). hist(aves$Observador.1, xlab = &quot;Observações&quot;, ylab = &quot;Frequência&quot;, main = &quot;Observador 1&quot;, ylim = c(0, 10), xlim = c(18, 25)) Figura 4.14: Histograma com a frequência de observações de aves, por 20 dias, pelo observador 1 hist(aves$Observador.2, xlab = &quot;Observações&quot;, ylab = &quot;Frequência&quot;, main = &quot;Observador 2&quot;, ylim = c(0, 10), xlim = c(18, 25)) Figura 4.15: Histograma com a frequência de observações de aves, por 20 dias, pelo observador 2 Agora vejamos o boxplot, repare que neste caso a sua escrita difere um pouco do que fizemos nos exemplos anteriores, pois agora inserimos a coluna referente ao “Observador 1” e depois a coluna referente ao “Observador 2” ((Figura 4.16). Isso se deve ao fato da planilha estar estruturada de forma diferente das anteriores. Não utilizamos o til (~), mas inserimos o nome da planilha seguido pelo operador matemático “$” (cifrão) mais o nome da variável quantitativa que queremos representar. Inserimos também outros 2 argumentos que são “names” com dois nomes concatenados pela função c() que representam as variaveis quantitativas na ordem em que foram inseridas e o argumento “ylab” que dá nome ao eixo y. boxplot(aves$Observador.1, aves$Observador.2, names = c(&quot;Observador 1&quot;, &quot;Observador 2&quot;), ylab = &quot;Frequência de Observações&quot;) Figura 4.16: Boxplot com as frequências de observações de aves por 20 dias de 2 observadores. Vejamos o o resumo de nossos dados utilizando a função padrão do R summary. summary(aves) ## Dia Observador.1 Observador.2 ## Dia 1 : 1 Min. :18.00 Min. :18.00 ## Dia 10 : 1 1st Qu.:20.00 1st Qu.:20.00 ## Dia 11 : 1 Median :21.00 Median :22.00 ## Dia 12 : 1 Mean :20.70 Mean :21.90 ## Dia 13 : 1 3rd Qu.:21.25 3rd Qu.:23.25 ## Dia 14 : 1 Max. :22.00 Max. :25.00 ## (Other):14 Como podemos notar, devido a organização dos dados na planilha a função summary() já resume de maneira adequada nossos dados. Em relação ao resumo dos nossos dados temos que o observador 2 contabilizou um número maior de aves que o observador 1, mas será que a diferença nos pares de observações é zero? De outra maneira, será que a média da diferença entre os pares de obsevações de aves entre os observadores é similar? O teste-t pareado não apresenta pressuposto quanto aos dados, porém como ele avalia a diferença entre dois grupos o pressuposto requerido é a normalidade da diferença dos dados. Vamos a nossa avaliação do pressuposto. Primeiro vamos criar um objeto que consiste na diferença entre observadores diferenca &lt;- aves$Observador.1 - aves$Observador.2 Agora vamos realizar o teste de normalidade da diferença. shapiro.test(diferenca) ## ## Shapiro-Wilk normality test ## ## data: diferenca ## W = 0.98173, p-value = 0.9544 De acordo com o teste de Shapiro os dados são normais. Vamos a nossa avaliação pelo teste-t. t.test(aves$Observador.1, aves$Observador.2, paired = TRUE, conf.level = 0.95) ## ## Paired t-test ## ## data: aves$Observador.1 and aves$Observador.2 ## t = -2.1608, df = 19, p-value = 0.04369 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.36237491 -0.03762509 ## sample estimates: ## mean of the differences ## -1.2 Para a execução do teste-t pareado 2 diferenças podem ser notadas na escrita da função, em relação ao teste-t para duas amostras. A primeira consiste no fato de que não utilizamos o til (~), mas sim as variáveis referentes as observações e a segunda é o argumento “paired” que tem valor lógico (ou seja, TRUE ou FALSE) e indicamos ele como “TRUE” (verdadeiro). Quanto ao resultado podemos notar que nos é informado que o teste consiste num teste-t pareado e que o número de aves observadas pelos observadores é ligeiramente diferente a um nível de confiança de 95%, visto que o p-value é próximo à 0,05 e na última linha nos é indicado que a média da diferença das observações é de -1,2. Em outras palavras o teste nos diz que a média das diferenças nas observações de aves é diferente de 018, portanto rejeitamos a hipótese nula. Assim como fizemos anteriormente vamos olhar o resultado em relação a distribuição função de densidade do teste-t (Figura 4.17). Só devemos lembrar de carregar o pacote webr se ainda não foi carregado. library(webr) plot(t.test(aves$Observador.1, aves$Observador.2, paired = TRUE, conf.level = 0.95)) Figura 4.17: Curva de densidade representando o valor do teste-t bicaudal para comparação do número de observações de aves por 2 observadores distintos a um intervalo de confiança de 95%. O ponto azul indica o valor do teste. Neste teste em particular como trabalhamos com a diferença entre as observações podemos usar o gráfico de barras para graficar essa diferença. Para o gráfico indicaremos em seu comando que faremos um gráfico de barras onde o que será plotado é o objeto que consiste na diferença entre o número de observação de aves entre observadores (objeto: diferenca) (Figura 4.18). O argumento “xlab” indica o nome do eixo x, “ylab” o nome do eixo y, “main” indica o título do gráfico, “names.arg” indica a coluna referente aos nomes das barras (que são os dias), “las” indica se os nomes das barras serão plotados na horizontal ou vertical (o valor 2 indica vertical), “ylim” indica os limites do eixo y e “cex.names” indica o tamanho da letra dos nomes das barras. barplot(diferenca, xlab = &quot;Dias&quot;, ylab = &quot;Diferença entre observadores (Observador 1 - Observador 2)&quot;, main = &quot;Meu gráfico&quot;, names.arg = aves$Dia, las = 2, ylim = c(-7.3, 7.3), cex.names = 0.9) Figura 4.18: Diferença entre o número de observações de aves, por dia, para cada observador As barras para o lado positivo do eixo y indica uma maior observação de aves pelo “Observador 1” e as barras para baixo indicam um maior número de observações de aves pelo “Observador 2”. Vamos indicar isso no gráfico por meio da função mtext() (Figura 4.19). barplot(diferenca, xlab = &quot;Dias&quot;, ylab = &quot;Diferença entre observadores (Observador 1 - Observador 2)&quot;, main = &quot;Meu gráfico&quot;, names.arg = aves$Dia, las = 2, ylim = c(-7.3, 7.3), cex.names = 0.9) mtext(at = 4, line = -2, text = &quot;Observador 1&quot;, side = 3) mtext(at = 4, line = -2, text = &quot;Observador 2&quot;, side = 1) Figura 4.19: Diferença entre o número de observações de aves, por dia, para cada observador. Como podem visualizar a função mtext() indicou os nomes “Observador 1” e “Observador 2” no lado do gráfico que os representa. O argumento “at” indica a posição em relação ao eixo x, “line” indica a posição em relação ao eixo y, “text” indica o que será plotado e side indica o lado da janela gráfica onde o texto será plotado (3 é na parte superior e 1 na inferior). 4.4 Considerações Durante nosso percurso neste capítulo realizamos etapas que consistem na construção dos dados no próprio programa do R e a condução da análise do teste-t para uma amostra, para duas amostras e pareado. A construção dos dados tem como resultado final um objeto similar a planilha de dados que deve ser importada, caso já a tenha preparado, e juntamente com a sumarização dos dados temos as mesmas etapas trabalhadas no capítulo anterior. A partir daí temos a condução dos pressupostos normalidade e homocedasticidade e da análise estatística o teste-t. Um resumo gráfico das etapas pode ser observado aqui (Figura 4.20). Figura 4.20: Resumo dos passos abordados no capítulo: da preparação dos dados até a análise dos dados. 4.5 Exercícios Durante uma aula de biologia marinha, os alunos foram até um costão rochoso e realizaram amostragens não destrutivas da comunidade de organismos presentes na região entre marés. Foi observado ao final das amostragens os percentuais de algas a cada metro ao longo de um transecto de 20m. O resultado dos dados de algas, obtidos pelos alunos, pode ser obtido pela execução dos comandos abaixo: set.seed(45) algas &lt;- (rnorm(n = 20, sd = 0.1) + runif(n = 20, min = 0, max = 0.8))*100 Na mesma aula oferecida à turma anterior, um ano antes no mesmo costão, foi observado uma média geral da população de algas de 52%. Usando seus conhecimentos sobre teste-t. Podemos dizer que o percentual de algas se manteve? Considere os níveis de confiança de 95% e 99%. Qual foi o intervalo de confiança de 95% e 99% obtido no exercício anterior? A média de algas observada pela turma anterior (52%) encontra-se dentro ou fora destes intervalos? Considerando o exemplo do exercício 1. Em um segundo momento a professora disponibilizou a turma os dados de algas obtidos pela turma anterior. Uma tabela com os dados de ambas as turmas pode ser obtido executando os comandos abaixo: turmas &lt;- c(rep(x = &quot;turma.1&quot;, 20), rep(x = &quot;turma.2&quot;, 20)) set.seed(44) algas.turma1 &lt;- (rnorm(n = 20, sd = 0.05) + runif(n = 20, min = 0.30, max = 0.83))*100 set.seed(45) algas.turma2 &lt;- (rnorm(n = 20, sd = 0.1) + runif(n = 20, min = 0, max = 0.8))*100 cobertura &lt;- c(algas.turma1, algas.turma2) algas.duasamostras &lt;- as.data.frame(cbind(turmas, round(cobertura, 2))) colnames(algas.duasamostras) &lt;- c(&quot;turmas&quot;, &quot;cobertura&quot;) algas.duasamostras$turma &lt;- as.factor(algas.duasamostras$turmas) algas.duasamostras$cobertura &lt;- as.numeric(algas.duasamostras$cobertura) rm(list = &quot;turmas&quot;, &quot;algas.turma1&quot;, &quot;algas.turma2&quot;, &quot;cobertura&quot;) Considerando, agora, o objeto algas.duasamostras. O que podemos dizer sobre o percentual de cobertura de algas, obtido por ambas as turmas, para os níveis de confiança de 95% e 99%? OBS: Converta as variáveis e conduza os pressupostos, se necessário. Utilizando o objeto algas.duasamostras, do exercício anterior, realize um gráfico que demonstre o percentual obtido por cada turma contendo o desvio padrão. Dois observadores independentes foram ao mesmo costão rochoso, anterior quantificaram a cobertura algal por metro, nos 20 metros do costão. Os dados obtidos por ambos os observadores pode ser obtido executando os comandos abaixo. set.seed(235) algas.observador1 &lt;- (rnorm(n = 20, sd = 0.05) + runif(n = 20, min = 0.30, max = 0.83))*100 set.seed(236) algas.observador2 &lt;- (rnorm(n = 20, sd = 0.15) + runif(n = 20, min = 0.20, max = 0.80))*100 posicao &lt;- seq(1:20) algas.pareado &lt;- as.data.frame(cbind(posicao, round(algas.observador1, 2), round(algas.observador2, 2))) colnames(algas.pareado) &lt;- c(&quot;posicao&quot;, &quot;observador.1&quot;, &quot;observador.2&quot;) algas.pareado$observador.1 &lt;- as.numeric(algas.pareado$observador.1) algas.pareado$observador.2 &lt;- as.numeric(algas.pareado$observador.2) rm(list = &quot;algas.observador1&quot;, &quot;algas.observador2&quot;, &quot;posicao&quot;) De acordo com este novo objeto algas.pareado, o que podemos dizer sobre o percentual de cobertura de algas, em relação aos observadores para cada posição do costão, para os níveis de confiança de 95% e 99%? OBS: Conduza os pressupostos se necessário. Consiste na subtração do nível de significância (alfa) de 1. Ou seja: nível de confiança = 1 - alfa.↩ por padrão o valor lógico para este argumento é FALSE e a função t.test() realiza o teste de Welch)↩ Zero (0) indica igualdade no número de observações.↩ "],["resolução-dos-exercícios.html", "A Resolução dos Exercícios A.1 Capítulo 1 A.2 Capítulo 2 A.3 Capítulo 3 A.4 Capítulo 4", " A Resolução dos Exercícios A.1 Capítulo 1 b a c b e A.2 Capítulo 2 e d d b c A.3 Capítulo 3 observados &lt;- c(550, 620) esperados &lt;- c(0.5, 0.5) chisq.test(x = observados, p = esperados) ## ## Chi-squared test for given probabilities ## ## data: observados ## X-squared = 4.188, df = 1, p-value = 0.04071 De acordo com o resultado obtido p-value &lt; 0,05, temos que o número decarros observados entre os períodos (manhã e tarde) são diferentes e de acordo com os valores obtidos (550 carros no período da manha e 620 no período da tarde) corroboramos corroboramos a observação do pesquisador que no período da tarde há um maior trafego de carros. dados &lt;- matrix(data = c(200, 350, 230, 390), ncol = 2, byrow = TRUE) rownames(dados) &lt;- c(&quot;Manhã&quot;, &quot;Tarde&quot;) colnames(dados) &lt;- c(&quot;Quarta-feira&quot;, &quot;Sexta-feira&quot;) dados ## Quarta-feira Sexta-feira ## Manhã 200 350 ## Tarde 230 390 chisq.test(x = dados) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: dados ## X-squared = 0.039543, df = 1, p-value = 0.8424 De acordo com o resultado obtido p-value &gt; 0,05, não podemos refutar a hipótese nula (H0: As proporções relativas da circulação de carros na praia por período do dia são independentes dos dias da semana avaliados). resultado &lt;- chisq.test(x = dados) prop.table(dados, margin = 2)*100 ## Quarta-feira Sexta-feira ## Manhã 46.51163 47.2973 ## Tarde 53.48837 52.7027 O erro tipo 1 consiste em rejeitar a hipótese nula quando ela é verdadeira. O erro tipo 2 consiste em aceitar a hipótese nula quando ela é falsa. No exercício 2 o p-value obtido foi de 0,057, o que corrobora a hipótese nula (H0), a um nível de confiança de 95%. Neste caso, como o nível de significância é próximo do limite estabelecido, há o risco de cometer o erro tipo 2. tabela &lt;- prop.table(x = dados, margin = 2)*100 par(mar = c(4, 4, 4, 13)) barplot(tabela, xlab = &quot;Dia da semana&quot;, col = c(&quot;red&quot;, &quot;darkblue&quot;), legend.text = TRUE, args.legend = list(x = &quot;right&quot;, bty = &quot;n&quot;, inset = -0.5)) A.4 Capítulo 4 set.seed(45) algas &lt;- (rnorm(n = 20, sd = 0.1) + runif(n = 20, min = 0, max = 0.8))*100 t.test(x = algas, mu = 52, conf.level = 0.95) ## ## One Sample t-test ## ## data: algas ## t = -2.5378, df = 19, p-value = 0.02007 ## alternative hypothesis: true mean is not equal to 52 ## 95 percent confidence interval: ## 30.11439 49.89807 ## sample estimates: ## mean of x ## 40.00623 t.test(x = algas, mu = 52, conf.level = 0.99) ## ## One Sample t-test ## ## data: algas ## t = -2.5378, df = 19, p-value = 0.02007 ## alternative hypothesis: true mean is not equal to 52 ## 99 percent confidence interval: ## 26.48517 53.52730 ## sample estimates: ## mean of x ## 40.00623 A depender do nível de confiança podemos ter diferentes conclusões. Se considerarmos um nível de confiança de 95% podemos dizer que a média do percentual de algas obtido pela turma que foi de 40,01% difere da média obtida pela turma anterior que foi de 52% com um p-value de 0,02. Se considerarmos um nível de confiança de 99% podemos dizer que a média do percentual de algas obtido pela turma que foi de 40,01% é similar ao obtido pela turma anterior que foi de 52% com um p-value de 0,02. O intervalo de confiança para o nível de 95% encontra-se entre: 30,11 e 49,90. O valor de 52% encontra-se fora do intervalo de confiança; O intervalo de confiança para o nível de 99% encontra-se entre: 26,49 e 53,53. O valor de 52% encontra-se dentro do intervalo de confiança; shapiro.test(algas.duasamostras$cobertura [algas.duasamostras$turmas == &quot;turma.1&quot;]) ## ## Shapiro-Wilk normality test ## ## data: algas.duasamostras$cobertura[algas.duasamostras$turmas == &quot;turma.1&quot;] ## W = 0.91597, p-value = 0.08291 shapiro.test(algas.duasamostras$cobertura [algas.duasamostras$turmas == &quot;turma.2&quot;]) ## ## Shapiro-Wilk normality test ## ## data: algas.duasamostras$cobertura[algas.duasamostras$turmas == &quot;turma.2&quot;] ## W = 0.97976, p-value = 0.9309 bartlett.test(cobertura ~ turmas, data = algas.duasamostras) ## ## Bartlett test of homogeneity of variances ## ## data: cobertura by turmas ## Bartlett&#39;s K-squared = 2.2754, df = 1, p-value = 0.1314 t.test(cobertura ~ turmas, data = algas.duasamostras, var.equal = TRUE, conf.level = 0.95) ## ## Two Sample t-test ## ## data: cobertura by turmas ## t = 2.0497, df = 38, p-value = 0.04734 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.146276 23.522724 ## sample estimates: ## mean in group turma.1 mean in group turma.2 ## 51.8420 40.0075 t.test(cobertura ~ turmas, data = algas.duasamostras, var.equal = TRUE, conf.level = 0.99) ## ## Two Sample t-test ## ## data: cobertura by turmas ## t = 2.0497, df = 38, p-value = 0.04734 ## alternative hypothesis: true difference in means is not equal to 0 ## 99 percent confidence interval: ## -3.821193 27.490193 ## sample estimates: ## mean in group turma.1 mean in group turma.2 ## 51.8420 40.0075 Ambas os conjuntos de dados demonstraram normalidade, pelo teste de Shapiro-Wilks, assim como homocedasticidade das variâncias, pelo teste de Bartlett. O teste-t fornece um p-value de 0,04734. Ao considerar o nível de confiança de 95% temos o valor no limiar da rejeição da hipótese nula. Se considerarmos o p-value como &lt;0,05 podemos refutar a hipótese nula e dizer que a cobertura algal obtida pelas duas turmas é diferente. Contudo ao nível de confiança de 99% podemos corroborar a hipótese nula e dizer que a cobertura algal observada por ambas as turmas são similares, embora a média de cobertura obtida pela primeira turma seja 11% menor. library(Rmisc) resultado &lt;- summarySE(data = algas.duasamostras, measurevar = &quot;cobertura&quot;, groupvars = &quot;turmas&quot;) grafico &lt;- barplot(formula = cobertura ~ turmas, data = resultado, beside = TRUE, col = c(&quot;green&quot;, &quot;blue&quot;), ylim = c(0, 100), ylab = &quot;Cobertura algal (%)&quot;, legend.text = TRUE) arrows(x0 = grafico, y0 = resultado$cobertura + resultado$sd, y1 = resultado$cobertura - resultado$sd, code = 3, angle = 90, length = 0.05) diferenca &lt;- algas.pareado$observador.1 - algas.pareado$observador.2 shapiro.test(diferenca) ## ## Shapiro-Wilk normality test ## ## data: diferenca ## W = 0.96839, p-value = 0.7206 t.test(algas.pareado$observador.1, algas.pareado$observador.2, paired = TRUE, conf.level = 0.95) ## ## Paired t-test ## ## data: algas.pareado$observador.1 and algas.pareado$observador.2 ## t = 0.91619, df = 19, p-value = 0.3711 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -7.052518 18.033518 ## sample estimates: ## mean of the differences ## 5.4905 t.test(algas.pareado$observador.1, algas.pareado$observador.2, paired = TRUE, conf.level = 0.99) ## ## Paired t-test ## ## data: algas.pareado$observador.1 and algas.pareado$observador.2 ## t = 0.91619, df = 19, p-value = 0.3711 ## alternative hypothesis: true difference in means is not equal to 0 ## 99 percent confidence interval: ## -11.65443 22.63543 ## sample estimates: ## mean of the differences ## 5.4905 A diferença entre os observadores mostrou normalidade para o teste de Shapiro-Wilks e independente do nível de confiança adotado (95% ou 99%) a média da diferença foi proxima a 0, indicando similaridade entre as observações. Corroborando, portanto, a hipótese nula (H0), de que não há diferença entre os observadores. "],["references.html", "References", " References "]]
